{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKidbdKZMOGv"
   },
   "source": [
    "이 자료는 위키독스 딥 러닝을 이용한 자연어 처리 입문의 정수 인코딩 챕터 튜토리얼 자료입니다.  \n",
    "\n",
    "링크 : https://wikidocs.net/31766"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7xwJrRARWcO"
   },
   "source": [
    "## **02-06 정수 인코딩(Integer Encoding)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNSGKXTjRZF2"
   },
   "source": [
    "컴퓨터는 텍스트보다는 숫자를 더 잘 처리 할 수 있습니다. 이를 위해 자연어 처리에서는 텍스트를 숫자로 바꾸는 여러가지 기법들이 있습니다. 그리고 그러한 기법들을 본격적으로 적용시키기 위한 첫 단계로 **각 단어를 고유한 정수에 맵핑(mapping)** 시키는 전처리 작업이 필요할 때가 있습니다.\n",
    "\n",
    "예를 들어 갖고 있는 텍스트에 단어가 5,000개가 있다면, 5,000개의 단어들 각각에 1번부터 5,000번까지 단어와 맵핑되는 고유한 정수. 다른 표현으로는 인덱스를 부여합니다. 가령, book은 150번, dog는 171번, love는 192번, books는 212번과 같이 숫자가 부여됩니다. 인덱스를 부여하는 방법은 여러 가지가 있을 수 있는데 랜덤으로 부여하기도 하지만, **보통은 단어 등장 빈도수를 기준으로 정렬한 뒤에** 부여합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICca7ZETRbpH"
   },
   "source": [
    "---\n",
    "### **1. 정수 인코딩(Integer Encoding)**\n",
    "---\n",
    "왜 이러한 작업이 필요한 지에 대해서는 뒤에서 원-핫 인코딩 실습이나, 워드 임베딩 챕터 등에서 알아보기로 하고 여기서는 어떤 과정으로 단어에 정수 인덱스를 부여하는지에 대해서만 정리하겠습니다.\n",
    "\n",
    "단어에 정수를 부여하는 방법 중 하나로 **단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고,** **빈도수가 높은 순서대로** 차례로 낮은 숫자부터 정수를 부여하는 방법이 있습니다. 이해를 돕기위해 단어의 빈도수가 적당하게 분포되도록 의도적으로 만든 텍스트 데이터를 가지고 실습해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZolhMgOL1RG"
   },
   "source": [
    "#### **1) dictionary 사용하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1675059060451,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "P41XK9iPpdjw"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675059060451,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "brBb-QZVpfSH"
   },
   "outputs": [],
   "source": [
    "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Auniew2TSTy-"
   },
   "source": [
    "우선 여러 문장이 함께 있는 텍스트 데이터로부터 **문장 토큰화** 를 수행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675059062882,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "qxZ2dSPUpg62",
    "outputId": "e5be2691-0915-4ae5-c928-5de874c4d7f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A barber is a person.',\n",
       " 'a barber is good person.',\n",
       " 'a barber is huge person.',\n",
       " 'he Knew A Secret!',\n",
       " 'The Secret He Kept is huge secret.',\n",
       " 'Huge secret.',\n",
       " 'His barber kept his word.',\n",
       " 'a barber kept his word.',\n",
       " 'His barber kept his secret.',\n",
       " 'But keeping and keeping such a huge secret to himself was driving the barber crazy.',\n",
       " 'the barber went up a huge mountain.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 문장 토큰화\n",
    "sentences = sent_tokenize(raw_text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St9x3umoSY0O"
   },
   "source": [
    "기존의 텍스트 데이터가 문장 단위로 토큰화 된 것을 확인할 수 있습니다. 이제 **정제 작업** 과 **정규화 작업** 을 병행하며, **단어 토큰화** 를 수행합니다. 여기서는 단어들을 **소문자화** 하여 단어의 개수를 통일시키고, **불용어와 단어 길이가 2이하인 경우에 대해서 단어를 일부 제외** 시켜주었습니다. **텍스트를 수치화하는 단계** 라는 것은 본격적으로 자연어 처리 작업에 들어간다는 의미이므로, 단어가 텍스트일 때만 할 수 있는 최대한의 전처리를 끝내놓아야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675059065298,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "VfA5es2Xpht2",
    "outputId": "697ef47d-ad8b-4447-d58a-65a62468672e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    # 단어 토큰화\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in tokenized_sentence: \n",
    "        word = word.lower()        # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
    "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거한다.\n",
    "            if len(word) > 2:      # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다.\n",
    "                result.append(word) \n",
    "                if word not in vocab: \n",
    "                    vocab[word] = 0     \n",
    "                vocab[word] += 1\n",
    "    preprocessed_sentences.append(result) \n",
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-Q-dcqpSa9-"
   },
   "source": [
    "현재 vocab에는 **각 단어에 대한 빈도수**가 기록되어져 있습니다. vocab을 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1675059067929,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "4L5i-fJ1p-jv",
    "outputId": "ab1a30af-50a5-46d1-e75c-8c56982ef995"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 8,\n",
       " 'person': 3,\n",
       " 'good': 1,\n",
       " 'huge': 5,\n",
       " 'knew': 1,\n",
       " 'secret': 6,\n",
       " 'kept': 4,\n",
       " 'word': 2,\n",
       " 'keeping': 2,\n",
       " 'driving': 1,\n",
       " 'crazy': 1,\n",
       " 'went': 1,\n",
       " 'mountain': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHMg8bPCSd31"
   },
   "source": [
    "파이썬의 딕셔너리 구조로 단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장되어져 있습니다. vocab에 단어를 입력하면 빈도수를 리턴합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1675059069943,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "G6sBmoKnqJNM",
    "outputId": "ea0b7130-5b85-4402-bdf6-0882953db32d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['barber']   # barber의 빈도수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPZzj-ksSgrG"
   },
   "source": [
    "이제 **빈도수가 높은 순서대로 정렬** 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675059072391,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "ThZt6GvMqKWG",
    "outputId": "0fe15104-1ca6-438a-f631-5cde4fb6820d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8),\n",
       " ('secret', 6),\n",
       " ('huge', 5),\n",
       " ('kept', 4),\n",
       " ('person', 3),\n",
       " ('word', 2),\n",
       " ('keeping', 2),\n",
       " ('good', 1),\n",
       " ('knew', 1),\n",
       " ('driving', 1),\n",
       " ('crazy', 1),\n",
       " ('went', 1),\n",
       " ('mountain', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "vocab_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlZwMt24SijR"
   },
   "source": [
    "**높은 빈도수**를  가진 단어일수록 **낮은 정수** 를 부여합니다. 정수는 **1부터** 부여합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675059074696,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "pNstQGVRqLqG",
    "outputId": "fdc1d22b-eef2-4154-961a-cdebe935d384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab_sorted :\n",
    "    if frequency > 1 : # 빈도수가 작은 단어는 제외.\n",
    "        i = i + 1\n",
    "        word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xkYXnZVSmQI"
   },
   "source": [
    "1의 인덱스를 가진 단어가 가장 빈도수가 높은 단어가 됩니다. 그리고 이러한 작업을 수행하는 동시에 각 단어의 빈도수를 알 경우에만 할 수 있는 전처리인 **빈도수가 적은 단어를 제외시키는 작업** 을 수행했습니다. 등장 빈도가 낮은 단어는 자연어 처리에서 의미를 가지지 않을 가능성이 높기 때문입니다. 여기서는 빈도수가 1인 단어들은 전부 제외시켰습니다.\n",
    "\n",
    "자연어 처리를 하다보면, 텍스트 데이터에 있는 단어를 모두 사용하기 보다는 **빈도수가 가장 높은 n개의 단어만 사용** 하고 싶은 경우가 많습니다. 위 단어들은 빈도수가 높은 순으로 낮은 정수가 부여되어져 있으므로 빈도수 상위 n개의 단어만 사용하고 싶다고하면 vocab에서 정수값이 1부터 n까지인 단어들만 사용하면 됩니다. 여기서는 상위 5개 단어만 사용한다고 가정하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675059076767,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "JVAiOfO1qPJP",
    "outputId": "3ddc7464-bc48-4064-a97d-5464b9f4ef77"
   },
   "outputs": [],
   "source": [
    "## 빈도수 상위 5개 단어만 남기고 나머지 단어는 제거\n",
    "\n",
    "## 아래 코드로 대체하는 것이 나을 듯...\n",
    "# vocab_size = 5\n",
    "# words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
    "# for w in words_frequency:\n",
    "#     del word_to_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
    "# print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 빈도수 상위 5개 단어만 남기고 나머지 단어는 제거 \n",
    "vocab_size = 5\n",
    "{word:index for word, index in word_to_index.items() if index <= vocab_size} ## 정제된 단어장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word', 'keeping']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 생략된 단어들(토큰)\n",
    "[word for word, index in word_to_index.items() if index >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAwuBXWASpud"
   },
   "source": [
    "word_to_index에는 빈도수가 높은 상위 5개의 단어만 저장되었습니다. word_to_index를 사용하여 단어 토큰화가 된 상태로 저장된 sentences에 있는 각 단어를 정수로 바꾸는 작업을 하겠습니다.\n",
    "\n",
    "예를 들어 sentences에서 첫번째 문장은 ['barber', 'person']이었는데, 이 문장에 대해서는 [1, 5]로 인코딩합니다. 그런데 두번째 문장인 ['barber', 'good', 'person']에는 더 이상 word_to_index에는 존재하지 않는 단어인 'good'이라는 단어가 있습니다.\n",
    "\n",
    "이처럼 단어 집합에 존재하지 않는 단어들이 생기는 상황을 **Out-Of-Vocabulary(단어 집합에 없는 단어)** 문제라고 합니다. 약자로 **'OOV 문제'** 라고도 합니다. word_to_index에 'OOV'란 단어를 새롭게 추가하고, 단어 집합에 없는 단어들은 'OOV'의 인덱스로 인코딩하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675059079363,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "s2E0n6WEqau3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'OOV': 8}\n"
     ]
    }
   ],
   "source": [
    "word_to_index['OOV'] = len(word_to_index) + 1\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzpjdgc1S1z_"
   },
   "source": [
    "이제 word_to_index를 사용하여 sentences의 모든 단어들을 맵핑되는 정수로 인코딩하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675059083272,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "brBkMS4DqdOP",
    "outputId": "263e1f09-6fb0-4d10-fc08-463c7346615a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [8, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 8, 1, 8], [1, 8, 3, 8]]\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = []   # 문서\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoded_sentence = []  # 문장\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            encoded_sentence.append(word_to_index[word])\n",
    "        except KeyError:\n",
    "            encoded_sentence.append(word_to_index['OOV'])\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbMTe3vcL4a6"
   },
   "source": [
    "#### **2) Counter 사용하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675059086107,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "IZkI_y03qsfY"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1675059089095,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "mlVjBnQsq6r-",
    "outputId": "4eb0ed31-5167-4c5b-9702-d855cf2c53de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675059090339,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "ICOgMShDq9T-",
    "outputId": "be5530f1-1b3b-4723-dc98-2c41158fe854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# words = np.hstack(preprocessed_sentences)으로도 수행 가능.\n",
    "all_words_list = sum(preprocessed_sentences, [])\n",
    "print(all_words_list) # 모든 단어들을 한 줄 리스트로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675059092193,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "Tr-RD6mdrNW9",
    "outputId": "7896e762-e252-44d7-afa0-5af7f5672b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "collections.Counter"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파이썬의 Counter 모듈을 이용하여 단어의 빈도수 카운트\n",
    "vocab = Counter(all_words_list)  # word_to_index\n",
    "print(vocab)\n",
    "type(vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675059094615,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "riXUzdzJrRe4",
    "outputId": "a82783b3-3af7-48e4-c79a-09e1cd9d23b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675059097303,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "H9Rk9wBurSxo",
    "outputId": "328af2d8-e874-4754-87d2-6f62a19a2e7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 빈도수 높은 상위 5개 단어만 추출\n",
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1675059100995,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "K4_HjiI8rTmf",
    "outputId": "6392e22e-990c-47ca-ba9e-fa70598da710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "### 가장 많이 등장한 단어부터 1 이후 정수 부여\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab :\n",
    "    i = i + 1\n",
    "    word_to_index[word] = i\n",
    "    \n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLHS225wL-F5"
   },
   "source": [
    "#### **3) NLTK의 FreqDist 사용하기**\n",
    "NLTK에서는 빈도수 계산 도구인 FreqDist()를 지원합니다. 위에서 사용한 Counter()랑 같은 방법으로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge',\n",
       "       'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret',\n",
       "       'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept',\n",
       "       'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge',\n",
       "       'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge',\n",
       "       'mountain'], dtype='<U8')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.hstack(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, ...})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "f_vocab = FreqDist(np.hstack(preprocessed_sentences))\n",
    "f_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwdriHLZTjEQ"
   },
   "source": [
    "단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장되어져 있습니다. vocab에 단어를 입력하면 빈도수를 리턴합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(f_vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UzaiV3EToq3"
   },
   "source": [
    "barber란 단어가 총 8번 등장하였습니다. most_common()는 상위 빈도수를 가진 주어진 수의 단어만을 리턴합니다. 이를 사용하여 등장 빈도수가 높은 단어들을 원하는 개수만큼만 얻을 수 있습니다. 등장 빈도수 상위 5개의 단어만 단어 집합으로 저장해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1675059151312,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "E-ObWwGYrfUv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = f_vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrLQwCA2TrdV"
   },
   "source": [
    "앞서 Counter()를 사용했을 때와 결과가 같습니다. 이전 실습들과 마찬가지로 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여합니다. 그런데 이번에는 enumerate()를 사용하여 좀 더 짧은 코드로 인덱스를 부여하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOSMtWCQrlYf",
    "outputId": "d19c2ce1-4c47-4936-b098-d408f9c043a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKyLDNzLTuvF"
   },
   "source": [
    "위와 같이 인덱스를 부여할 때는 enumerate()를 사용하는 것이 편리합니다. enumerate()에 대해서 간단히 소개해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnfqyhEhMHif"
   },
   "source": [
    "#### **4) enumerate 이해하기**\n",
    "enumerate()는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스를 순차적으로 함께 리턴한다는 특징이 있습니다. 간단한 예제를 통해 enumerate()를 이해해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAIXuuDirtuv",
    "outputId": "87c89945-7950-4f53-b28b-c65b77bd83bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : a, index: 0\n",
      "value : b, index: 1\n",
      "value : c, index: 2\n",
      "value : d, index: 3\n",
      "value : e, index: 4\n"
     ]
    }
   ],
   "source": [
    "test_input = ['a', 'b', 'c', 'd', 'e']\n",
    "for index, value in enumerate(test_input): # 입력의 순서대로 0부터 인덱스를 부여함.\n",
    "  print(\"value : {}, index: {}\".format(value, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQWrFIvJT0Ge"
   },
   "source": [
    "위의 출력 결과는 리스트의 모든 토큰에 대해서 인덱스가 순차적으로 증가되며 부여된 것을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pS_DRLtMJl7"
   },
   "source": [
    "---\n",
    "### **2. 케라스(Keras)의 텍스트 전처리**\n",
    "---\n",
    "케라스(Keras)는 기본적인 전처리를 위한 도구들을 제공합니다. 때로는 **정수 인코딩** 을 위해서 케라스의 전처리 도구인 토크나이저를 사용하기도 하는데, 사용 방법과 그 특징에 대해서 이해해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "XNyMMfEBrz1n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 토큰화된 문장장\n",
    "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "BvjYhEkdryBf"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oi1mnQhNT8Al"
   },
   "source": [
    "단어 토큰화까지 수행된 앞서 사용한 텍스트 데이터와 동일한 데이터를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xVxlasf6r7p9"
   },
   "outputs": [],
   "source": [
    "## keras의 Tokenizer class로 단어 집합 생성\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "## fit_on_texts() : 빈도수 기반의 단어 집합 생성\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwsywA4OT9-1"
   },
   "source": [
    "`fit_on_texts` 는 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여하는데, 정확히 앞서 설명한 정수 인코딩 작업이 이루어진다고 보면됩니다. 각 단어에 인덱스가 어떻게 부여되었는지를 보려면, `word_index`를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WySIFxB3r-JP",
    "outputId": "32b1ed05-95e6-4210-afed-71e1a50350f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1,\n",
       " 'secret': 2,\n",
       " 'huge': 3,\n",
       " 'kept': 4,\n",
       " 'person': 5,\n",
       " 'word': 6,\n",
       " 'keeping': 7,\n",
       " 'good': 8,\n",
       " 'knew': 9,\n",
       " 'driving': 10,\n",
       " 'crazy': 11,\n",
       " 'went': 12,\n",
       " 'mountain': 13}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index  # 등장 횟수 순서로 인덱스 부여여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'barber',\n",
       " 2: 'secret',\n",
       " 3: 'huge',\n",
       " 4: 'kept',\n",
       " 5: 'person',\n",
       " 6: 'word',\n",
       " 7: 'keeping',\n",
       " 8: 'good',\n",
       " 9: 'knew',\n",
       " 10: 'driving',\n",
       " 11: 'crazy',\n",
       " 12: 'went',\n",
       " 13: 'mountain'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## key-value를 반대로... index_to_word\n",
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'barber',\n",
       " 2: 'secret',\n",
       " 3: 'huge',\n",
       " 4: 'kept',\n",
       " 5: 'person',\n",
       " 6: 'word',\n",
       " 7: 'keeping',\n",
       " 8: 'good',\n",
       " 9: 'knew',\n",
       " 10: 'driving',\n",
       " 11: 'crazy',\n",
       " 12: 'went',\n",
       " 13: 'mountain'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## word_index로 index_word 만들기\n",
    "{v: k for k, v in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kUwGWqEUAM9"
   },
   "source": [
    "각 단어의 빈도수가 높은 순서대로 인덱스가 부여된 것을 확인할 수 있습니다. 각 단어가 카운트를 수행하였을 때 몇 개였는지를 보고자 한다면 `word_counts` 를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPjn6u4qr_HP",
    "outputId": "b2ef9c90-b35f-4aaa-c5d3-8c88f3d813e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('barber', 8),\n",
       "             ('person', 3),\n",
       "             ('good', 1),\n",
       "             ('huge', 5),\n",
       "             ('knew', 1),\n",
       "             ('secret', 6),\n",
       "             ('kept', 4),\n",
       "             ('word', 2),\n",
       "             ('keeping', 2),\n",
       "             ('driving', 1),\n",
       "             ('crazy', 1),\n",
       "             ('went', 1),\n",
       "             ('mountain', 1)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 8,\n",
       " 'person': 3,\n",
       " 'good': 1,\n",
       " 'huge': 5,\n",
       " 'knew': 1,\n",
       " 'secret': 6,\n",
       " 'kept': 4,\n",
       " 'word': 2,\n",
       " 'keeping': 2,\n",
       " 'driving': 1,\n",
       " 'crazy': 1,\n",
       " 'went': 1,\n",
       " 'mountain': 1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuNt7ryXUDC9"
   },
   "source": [
    "`texts_to_sequences()`는 입력으로 들어온 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OF-ar0-Xr_9v",
    "outputId": "1fa95b3e-571b-45a6-d9ea-695fbdddf19b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 8, 5],\n",
       " [1, 3, 5],\n",
       " [9, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [7, 7, 3, 2, 10, 1, 11],\n",
       " [1, 12, 3, 13]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYDLmHdGUFe2"
   },
   "source": [
    "앞서 빈도수가 가장 높은 단어 n개만을 사용하기 위해서 `most_common()`을 사용했었습니다. 케라스 토크나이저에서는 `tokenizer = Tokenizer(num_words=숫자)`와 같은 방법으로 빈도수가 높은 상위 몇 개의 단어만 사용하겠다고 지정할 수 있습니다. 여기서는 1번 단어부터 5번 단어까지만 사용하겠습니다. 상위 5개 단어를 사용한다고 토크나이저를 재정의 해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnUMh8vYUH3t"
   },
   "source": [
    "num_words에서 `+1`을 더해서 값을 넣어주는 이유는 `num_words는 숫자를 0부터 카운트`합니다. 만약 5를 넣으면 0 ~ 4번 단어 보존을 의미하게 되므로 뒤의 실습에서 1번 단어부터 4번 단어만 남게됩니다. 그렇기 때문에 1 ~ 5번 단어까지 사용하고 싶다면 num_words에 숫자 5를 넣어주는 것이 아니라 5+1인 값을 넣어주어야 합니다.\n",
    "\n",
    "실질적으로 숫자 0에 지정된 단어가 존재하지 않는데도 케라스 토크나이저가 **숫자 0까지 단어 집합의 크기로 산정하는 이유** 는 자연어 처리에서 **패딩(padding)이라는 작업 때문**입니다. 이에 대해서는 뒤에 다루게 되므로 여기서는 케라스 토크나이저를 사용할 때는 숫자 0도 단어 집합의 크기로 고려해야한다고만 이해합시다.\n",
    "\n",
    "다시 word_index를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "renGtU4fsBwn"
   },
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0TXcbEPSsD-v",
    "outputId": "8936f1ba-44e2-4150-e430-fd61b5fbf802"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1,\n",
       " 'secret': 2,\n",
       " 'huge': 3,\n",
       " 'kept': 4,\n",
       " 'person': 5,\n",
       " 'word': 6,\n",
       " 'keeping': 7,\n",
       " 'good': 8,\n",
       " 'knew': 9,\n",
       " 'driving': 10,\n",
       " 'crazy': 11,\n",
       " 'went': 12,\n",
       " 'mountain': 13}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index  ## ?? 5개 단어만 사용하기로 했는데... 13개 모두 단어사전에 존재??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ar44bgrULZX"
   },
   "source": [
    "상위 5개의 단어만 사용하겠다고 선언하였는데 여전히 13개의 단어가 모두 출력됩니다. word_counts를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Po1w9WpNsFD_",
    "outputId": "e06c9de3-d427-4fb5-bc52-0548feaa0759"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('barber', 8),\n",
       "             ('person', 3),\n",
       "             ('good', 1),\n",
       "             ('huge', 5),\n",
       "             ('knew', 1),\n",
       "             ('secret', 6),\n",
       "             ('kept', 4),\n",
       "             ('word', 2),\n",
       "             ('keeping', 2),\n",
       "             ('driving', 1),\n",
       "             ('crazy', 1),\n",
       "             ('went', 1),\n",
       "             ('mountain', 1)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts  # 여전히 단어수 제한 안됨..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYoGXjf7UO-u"
   },
   "source": [
    "word_counts에서도 마찬가지로 13개의 단어가 모두 출력됩니다. 사실 **실제 적용은 texts_to_sequences를 사용할 때 적용** 이 됩니다. (oov는 제거됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0sbhhBKsGSH",
    "outputId": "43e5edc3-5235-47e3-ce37-7d7aed8f5bc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 5],\n",
       " [1, 3, 5],\n",
       " [2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4],\n",
       " [1, 4],\n",
       " [1, 4, 2],\n",
       " [3, 2, 1],\n",
       " [1, 3]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(preprocessed_sentences)  # 1~5만 사용됨 (이외의 단어 생략됨략됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_2Sl9TrURit"
   },
   "source": [
    "코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환하는데, 상위 5개의 단어만을 사용하겠다고 지정하였으므로 1번 단어부터 5번 단어까지만 보존되고 나머지 단어들은 제거된 것을 볼 수 있습니다. 경험상 굳이 필요하다고 생각하지는 않지만, 만약 word_index와 word_counts에서도 지정된 num_words만큼의 단어만 남기고 싶다면 아래의 코드도 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "IALRN6TDsJ4f"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17948a0HsQaH",
    "outputId": "b92d09c3-f6fe-44b5-ca5c-c022e7505b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "words_frequency = [word for word, index in tokenizer.word_index.items() if index >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
    "for word in words_frequency:\n",
    "    del tokenizer.word_index[word] # 해당 단어에 대한 인덱스 정보를 삭제\n",
    "    del tokenizer.word_counts[word] # 해당 단어에 대한 카운트 정보를 삭제\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdZFQ7luUXY9"
   },
   "source": [
    "케라스 토크나이저는 기본적으로 단어 집합에 없는 단어인 OOV에 대해서는 단어를 정수로 바꾸는 과정에서 아예 단어를 제거한다는 특징이 있습니다. 단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 Tokenizer의 인자 oov_token을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "JrfGD7f6stSf"
   },
   "outputs": [],
   "source": [
    "## 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2 \n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token='OOV') \n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NKrL8YvUagW"
   },
   "source": [
    "만약 **oov_token을 사용하기로 했다면** 케라스 토크나이저는 기본적으로 **'OOV'의 인덱스를 1로** 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['OOV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lrrRAUozsymH",
    "outputId": "cd6adb67-e223-4da7-9ebb-335009d93b8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OOV': 1,\n",
       " 'barber': 2,\n",
       " 'secret': 3,\n",
       " 'huge': 4,\n",
       " 'kept': 5,\n",
       " 'person': 6,\n",
       " 'word': 7,\n",
       " 'keeping': 8,\n",
       " 'good': 9,\n",
       " 'knew': 10,\n",
       " 'driving': 11,\n",
       " 'crazy': 12,\n",
       " 'went': 13,\n",
       " 'mountain': 14}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'OOV',\n",
       " 2: 'barber',\n",
       " 3: 'secret',\n",
       " 4: 'huge',\n",
       " 5: 'kept',\n",
       " 6: 'person',\n",
       " 7: 'word',\n",
       " 8: 'keeping',\n",
       " 9: 'good',\n",
       " 10: 'knew',\n",
       " 11: 'driving',\n",
       " 12: 'crazy',\n",
       " 13: 'went',\n",
       " 14: 'mountain'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGNhWqdsUde1"
   },
   "source": [
    "이제 코퍼스에 대해서 정수 인코딩을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qfcu_Y8s0_f",
    "outputId": "e0860a70-471b-4f3c-93fa-b2166d09b2af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6],\n",
       " [2, 1, 6],\n",
       " [2, 4, 6],\n",
       " [1, 3],\n",
       " [3, 5, 4, 3],\n",
       " [4, 3],\n",
       " [2, 5, 1],\n",
       " [2, 5, 1],\n",
       " [2, 5, 3],\n",
       " [1, 1, 4, 3, 1, 2, 1],\n",
       " [2, 1, 4, 1]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_texts = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "encoded_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e60l-0dxUgpu"
   },
   "source": [
    "빈도수 상위 5개의 단어는 2 ~ 6까지의 인덱스를 가졌으며, 그 외 단어 집합에 없는 'good'과 같은 단어들은 전부 'OOV'의 인덱스인 1로 인코딩되었습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl74Il4rUjoO"
   },
   "source": [
    "마지막 편집일시 : 2022년 11월 14일 2:42 오후"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barber person',\n",
       " 'barber OOV person',\n",
       " 'barber huge person',\n",
       " 'OOV secret',\n",
       " 'secret kept huge secret',\n",
       " 'huge secret',\n",
       " 'barber kept OOV',\n",
       " 'barber kept OOV',\n",
       " 'barber kept secret',\n",
       " 'OOV OOV huge secret OOV barber OOV',\n",
       " 'barber OOV huge OOV']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## encoded_texts로 문장 만들기...\n",
    "generated_text = []\n",
    "\n",
    "for enc_text in encoded_texts:\n",
    "    # print(enc_text)\n",
    "    s = []\n",
    "    for word in enc_text:\n",
    "        # print(tokenizer.index_word[word])\n",
    "        s.append(tokenizer.index_word[word])\n",
    "    generated_text.append(' '.join(s))\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
