{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z60Km7qsR1Lr"
   },
   "source": [
    "이 자료는 위키독스 '딥 러닝을 이용한 자연어 처리 입문'의 사전 훈련된 워드 임베딩 튜토리얼 자료입니다.  \n",
    "\n",
    "링크 : https://wikidocs.net/33793"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOy7f3aCmBlZ"
   },
   "source": [
    "## **09-08 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)**\n",
    "---\n",
    "  \n",
    "이번에는 **케라스의 임베딩 층(embedding layer)** 과 **사전 훈련된 워드 임베딩(pre-trained word embedding)** 을 가져와서 사용하는 것을 비교해봅니다. 자연어 처리를 하려고 할 때 갖고 있는 훈련 데이터의 단어들을 임베딩 층(embedding layer)을 구현하여 임베딩 벡터로 학습하는 경우가 있습니다. 케라스에서는 이를 Embedding()이라는 도구를 사용하여 구현합니다.\n",
    "\n",
    "위키피디아 등과 같은 방대한 코퍼스를 가지고 Word2vec, FastText, GloVe 등을 통해서 미리 훈련된 임베딩 벡터를 불러오는 방법을 사용하는 경우도 있습니다. 이는 현재 갖고 있는 훈련 데이터를 임베딩 층으로 처음부터 학습을 하는 방법과는 대조됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UEh3TIbQBnB"
   },
   "source": [
    "---\n",
    "### **1. 케라스 임베딩 층(Keras Embedding layer)**\n",
    "\n",
    "케라스는 훈련 데이터의 단어들에 대해 워드 임베딩을 수행하는 도구 Embedding()을 제공합니다. Embedding()은 인공 신경망 구조 관점에서 임베딩 층(embedding layer)을 구현합니다.  \n",
    "  \n",
    "#### **1) 임베딩 층은 룩업 테이블이다.**\n",
    "임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야 합니다.\n",
    "\n",
    "어떤 단어 → 단어에 부여된 고유한 정수값 → 임베딩 층 통과 → 밀집 벡터\n",
    "\n",
    "임베딩 층은 입력 정수에 대해 밀집 벡터(dense vector)로 맵핑하고 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련됩니다. 훈련 과정에서 단어는 모델이 풀고자하는 작업에 맞는 값으로 업데이트 됩니다. 그리고 이 밀집 벡터를 임베딩 벡터라고 부릅니다.\n",
    "\n",
    "정수를 밀집 벡터 또는 임베딩 벡터로 맵핑한다는 것은 어떤 의미일까요? 특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 임베딩 벡터 값을 가져오는 룩업 테이블이라고 볼 수 있습니다. 그리고 이 테이블은 단어 집합의 크기만큼의 행을 가지므로 모든 단어는 고유한 임베딩 벡터를 가집니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE7KqI_ImbYn"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exdD9sYcmeXo"
   },
   "source": [
    "위의 그림은 단어 great이 정수 인코딩 된 후 테이블로부터 해당 인덱스에 위치한 임베딩 벡터를 꺼내오는 모습을 보여줍니다. 위의 그림에서는 임베딩 벡터의 차원이 4로 설정되어져 있습니다. 그리고 단어 great은 정수 인코딩 과정에서 1,918의 정수로 인코딩이 되었고 그에 따라 단어 집합의 크기만큼의 행을 가지는 테이블에서 인덱스 1,918번에 위치한 행을 단어 great의 임베딩 벡터로 사용합니다. 이 임베딩 벡터는 모델의 입력이 되고, 역전파 과정에서 단어 great의 임베딩 벡터값이 학습됩니다.  \n",
    "  \n",
    "룩업 테이블의 개념을 이론적으로 우선 접하고, 처음 케라스를 배울 때 어떤 분들은 임베딩 층의 입력이 원-핫 벡터가 아니어도 동작한다는 점에 헷갈려 합니다. 앞서 NNLM이나 Word2Vec을 설명할 때 룩업 테이블을 언급하면서 입력을 원-핫 벡터로 가정하고 설명드렸기 때문인데, 케라스는 단어를 정수 인덱스로 바꾸고 원-핫 벡터로 변환 후 임베딩 층의 입력으로 사용하는 것이 아니라, 단어를 정수 인코딩까지만 진행 후 임베딩 층의 입력으로 사용하여 룩업 테이블 결과인 임베딩 벡터를 리턴합니다.  \n",
    "  \n",
    "케라스의 임베딩 층 구현 코드를 봅시다.\n",
    "  \n",
    "```python\n",
    "    vocab_size = 20000\n",
    "    output_dim = 128\n",
    "    input_length = 500\n",
    "\n",
    "    v = Embedding(vocab_size, output_dim, input_length=input_length)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1kurwLQmnA8"
   },
   "source": [
    "임베딩 층은 다음과 같은 세 개의 인자를 받습니다.\n",
    "\n",
    "**vocab_size** = 텍스트 데이터의 전체 단어 집합의 크기입니다.  \n",
    "**output_dim** = 워드 임베딩 후의 임베딩 벡터의 차원입니다.  \n",
    "**input_length** = 입력 시퀀스의 길이입니다. 만약 갖고있는 각 샘플의 길이가 500개이라면 이 값은 500입니다.  \n",
    "  \n",
    "Embedding()은 (number of samples, input_length)인 2D 정수 텐서를 입력받습니다. 이때 각 sample은 정수 인코딩이 된 결과로 정수 시퀀스입니다. Embedding()은 워드 임베딩 작업을 수행하고 (number of samples, input_length, embedding word dimentionality)인 3D 실수 텐서를 리턴합니다. 케라스의 임베딩 층(embedding layer)을 사용하는 실습을 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaCVzgG0m6Ez"
   },
   "source": [
    "#### **2) 임베딩 층 사용하기**\n",
    "문장의 긍, 부정을 판단하는 감성 분류 모델을 만들어봅시다. 문장과 레이블 데이터를 만들었습니다. 긍정인 문장은 레이블 1, 부정인 문장은 레이블이 0입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBEq_NokPhdN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Zs1zoLEPkkv"
   },
   "outputs": [],
   "source": [
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBXrSM-_nBQV"
   },
   "source": [
    "케라스의 토크나이저를 사용하여 단어 집합을 만들고 그 크기를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPO7VSsNPlYu",
    "outputId": "c00fdc91-b835-4b68-8cda-97405147845a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QuB-vO8nEIw"
   },
   "source": [
    "각 문장에 대해서 정수 인코딩을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_nGLh4jPn6G",
    "outputId": "4fbe017a-e054-4267-cca0-52699e7c5e91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
     ]
    }
   ],
   "source": [
    "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print(X_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXfUUKJVnIK3"
   },
   "source": [
    "가장 길이가 긴 문장의 길이를 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_9xEc4UPtkq",
    "outputId": "d657a90a-a9e9-45e8-de95-663365e6199b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in X_encoded)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXPXFEoJnLn5"
   },
   "source": [
    "최대 길이로 모든 샘플에 대해서 패딩을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ic5dberhPpQ-",
    "outputId": "904f031c-41f9-4e19-9ec5-6f6f1e11a3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  0  0]\n",
      " [ 7  8  0  0]\n",
      " [ 9 10  0  0]\n",
      " [11 12  0  0]\n",
      " [13  0  0  0]\n",
      " [14 15  0  0]]\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
    "y_train = np.array(y_train)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAXcyHI0nOFL"
   },
   "source": [
    "훈련 데이터에 대한 전처리가 끝났습니다. 전형적인 이진 분류 모델을 설계합니다. 출력층에 1개의 뉴런을 배치하고 활성화 함수로는 시그모이드 함수를, 그리고 손실 함수로 binary_crossentropy를 사용합니다. 그 후 100 에포크 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUVmsvRYPqkG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "embedding_dim = 4\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwhOlBrNPzt-",
    "outputId": "7fe0c840-48e2-4d04-80c1-41d7a42292e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 2s - loss: 0.6845 - acc: 0.5714\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.6832 - acc: 0.5714\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.6818 - acc: 0.5714\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.6805 - acc: 0.7143\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.6791 - acc: 0.7143\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.6777 - acc: 0.7143\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.6764 - acc: 0.7143\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.6750 - acc: 0.8571\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.6736 - acc: 0.8571\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.6723 - acc: 0.8571\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.6709 - acc: 0.8571\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.6695 - acc: 0.8571\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.6682 - acc: 0.8571\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 0.6668 - acc: 0.8571\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 0.6654 - acc: 0.8571\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 0.6640 - acc: 0.8571\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 0.6626 - acc: 0.8571\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 0.6612 - acc: 0.8571\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 0.6598 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 0.6583 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 0.6569 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 0.6555 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 0.6540 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 0.6526 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 0.6511 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 0.6497 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 0.6482 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 0.6467 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 0.6452 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 0.6437 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 0.6422 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 0.6406 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 0.6391 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 0.6376 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 0.6360 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 0.6345 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 0.6329 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 0.6313 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 0.6297 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 0.6281 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 0.6265 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 0.6249 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 0.6233 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 0.6216 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 0.6200 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 0.6183 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 0.6166 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 0.6150 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 0.6133 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 0.6116 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 0.6099 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 0.6081 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 0.6064 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 0.6047 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 0.6029 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 0.6012 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 0.5994 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 0.5976 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 0.5958 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 0.5941 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 0.5922 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 0.5904 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 0.5886 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 0.5868 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 0.5849 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 0.5831 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 0.5812 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 0.5794 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 0.5775 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 0.5756 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 0.5737 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 0.5718 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 0.5699 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 0.5680 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 0.5661 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 0.5642 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 0.5622 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 0.5603 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 0.5583 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 0.5564 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 0.5544 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 0.5524 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 0.5505 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 0.5485 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 0.5465 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 0.5445 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 0.5425 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 0.5405 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 0.5385 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 0.5364 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 0.5344 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 0.5324 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 0.5303 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 0.5283 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 0.5262 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 0.5242 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 0.5221 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 0.5201 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 0.5180 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 0.5159 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff4b0140dd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6FahIJ-nXMm"
   },
   "source": [
    "학습 과정에서 현재 각 단어들의 임베딩 벡터들의 값은 출력층의 가중치와 함께 학습됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4uw-hLxP_m-"
   },
   "source": [
    "---\n",
    "### **2. 사전 훈련된 GloVe 사용하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3xQUluPndQd"
   },
   "source": [
    "케라스의 Embedding()을 사용하여 처음부터 임베딩 벡터값을 학습하기도 하지만, 때로는 이미 훈련되어져 있는 워드 임베딩을 가져와서 이를 임베딩 벡터로 사용하기도 합니다. 훈련 데이터가 적은 상황이라면 케라스의 Embedding()으로 해당 문제를 풀기에 최적화 된 임베딩 벡터값을 얻는 것이 쉽지 않습니다. 이 경우 해당 문제에 특화된 것은 아니지만 보다 많은 훈련 데이터로 이미 Word2Vec이나 GloVe 등으로 학습되어져 있는 임베딩 벡터들을 사용하는 것이 성능의 개선을 가져올 수 있습니다.  \n",
    "  \n",
    "사전 훈련된 GloVe와 Word2Vec 임베딩을 사용해서 모델을 훈련시키는 실습을 진행해봅시다.  \n",
    "  \n",
    "* GloVe 다운로드 링크 : http://nlp.stanford.edu/data/glove.6B.zip\n",
    "* Word2Vec 다운로드 링크 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM  \n",
    "  \n",
    "훈련 데이터는 앞서 사용했던 데이터에 동일한 전처리까지 진행된 상태라고 가정하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6dQBG76QK9d"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve, urlopen\n",
    "import gzip\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMZIz7cpQGPH"
   },
   "outputs": [],
   "source": [
    "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
    "zf = zipfile.ZipFile('glove.6B.zip')\n",
    "zf.extractall() \n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6mt8jg2opga"
   },
   "source": [
    "glove.6B.100d.txt에 있는 모든 임베딩 벡터들을 불러옵니다. 파이썬의 자료구조 딕셔너리(dictionary)를 사용하며, 로드한 임베딩 벡터의 개수를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMNEmTj0QZ2R",
    "outputId": "59af40a2-69e7-4b1b-96be-95b80058fbe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000개의 Embedding vector가 있습니다.\n"
     ]
    }
   ],
   "source": [
    "embedding_dict = dict()\n",
    "\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "    \n",
    "    # 100개의 값을 가지는 array로 변환\n",
    "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
    "    embedding_dict[word] = word_vector_arr\n",
    "f.close()\n",
    "\n",
    "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0HztHcros18"
   },
   "source": [
    "총 40만개의 임베딩 벡터가 존재합니다. 임의의 단어 'respectable'의 임베딩 벡터값과 크기를 출력해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0czY3cKhQjYn",
    "outputId": "dfe746e7-c7d8-48e1-eb35-0942c6dc2d43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\n",
      "  0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\n",
      "  0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\n",
      "  0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\n",
      " -0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\n",
      " -0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\n",
      " -0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\n",
      " -0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\n",
      " -0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\n",
      " -0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\n",
      " -0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\n",
      "  0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\n",
      "  0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\n",
      "  0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\n",
      " -0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\n",
      " -0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\n",
      " -0.21616   -0.19187   -0.032502   0.38025  ]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(embedding_dict['respectable'])\n",
    "print(len(embedding_dict['respectable']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co3MUz7jow7d"
   },
   "source": [
    "벡터값이 출력되며 벡터의 차원 수는 100입니다. 풀고자 하는 문제의 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성합니다. 이 행렬의 값은 전부 0으로 채웁니다. 이 행렬에 사전 훈련된 임베딩 값을 넣어줄 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RtqXv34QkOu",
    "outputId": "34e96738-059d-4029-eed9-d2c5aa0b3336"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMJCW5xXo3KG"
   },
   "source": [
    "기존 데이터의 각 단어와 맵핑된 정수값을 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhAjkMDcQmAu",
    "outputId": "40a18cdc-4d26-4681-ea13-b86704cf28e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os62P6_qpMLz"
   },
   "source": [
    "단어 'great'의 맵핑된 정수는 2입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPLZvbnRRMWR",
    "outputId": "1fbaf675-54c3-4f8d-e883-2e1c38b3cdd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"단어 great의 맵핑된 정수 : \", tokenizer.word_index['great'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWyhMDI6o80J"
   },
   "source": [
    "사전 훈련된 GloVe에서 'great'의 벡터값을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wm3CIglpRR59",
    "outputId": "6b6ceab2-0c3c-4d70-ba49-4b3ae663e919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\n",
      " -0.41846   -0.58437   -0.77355   -0.87866   -0.37858   -0.18516\n",
      " -0.128     -0.20584   -0.22925   -0.42599    0.3725     0.26077\n",
      " -1.0702     0.62916   -0.091469   0.70348   -0.4973    -0.77691\n",
      "  0.66045    0.09465   -0.44893    0.018917   0.33146   -0.35022\n",
      " -0.35789    0.030313   0.22253   -0.23236   -0.19719   -0.0053125\n",
      " -0.25848    0.58081   -0.10705   -0.17845   -0.16206    0.087086\n",
      "  0.63029   -0.76649    0.51619    0.14073    1.019     -0.43136\n",
      "  0.46138   -0.43585   -0.47568    0.19226    0.36065    0.78987\n",
      "  0.088945  -2.7814    -0.15366    0.01015    1.1798     0.15168\n",
      " -0.050112   1.2626    -0.77527    0.36031    0.95761   -0.11385\n",
      "  0.28035   -0.02591    0.31246   -0.15424    0.3778    -0.13599\n",
      "  0.2946    -0.31579    0.42943    0.086969   0.019169  -0.27242\n",
      " -0.31696    0.37327    0.61997    0.13889    0.17188    0.30363\n",
      " -1.2776     0.044423  -0.52736   -0.88536   -0.19428   -0.61947\n",
      " -0.10146   -0.26301   -0.061707   0.36627   -0.95223   -0.39346\n",
      " -0.69183   -1.0426     0.28855    0.63056  ]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_dict['great'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVE6ypy7pei0"
   },
   "source": [
    "단어 집합의 모든 단어에 대해서 사전 훈련된 GloVe의 임베딩 벡터들을 맵핑한 후 'great'의 벡터값이 의도한 인덱스의 위치에 삽입되었는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tf5G45yGQn7u"
   },
   "outputs": [],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
    "    vector_value = embedding_dict.get(word)\n",
    "    if vector_value is not None:\n",
    "        embedding_matrix[index] = vector_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTJfjiwKphCb"
   },
   "source": [
    "embedding_matrix의 인덱스 2에서의 값을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0OnjVjPRXAk",
    "outputId": "aa4557cc-9d49-4373-abdc-b0635ef7723d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\n",
       "       -0.20558   , -0.41846001, -0.58437002, -0.77354997, -0.87866002,\n",
       "       -0.37858   , -0.18516   , -0.12800001, -0.20584001, -0.22925   ,\n",
       "       -0.42598999,  0.3725    ,  0.26076999, -1.07019997,  0.62915999,\n",
       "       -0.091469  ,  0.70348001, -0.4973    , -0.77691001,  0.66044998,\n",
       "        0.09465   , -0.44893   ,  0.018917  ,  0.33146   , -0.35021999,\n",
       "       -0.35789001,  0.030313  ,  0.22253001, -0.23236001, -0.19719   ,\n",
       "       -0.0053125 , -0.25848001,  0.58081001, -0.10705   , -0.17845   ,\n",
       "       -0.16205999,  0.087086  ,  0.63028997, -0.76648998,  0.51618999,\n",
       "        0.14072999,  1.01900005, -0.43136001,  0.46138   , -0.43584999,\n",
       "       -0.47567999,  0.19226   ,  0.36065   ,  0.78987002,  0.088945  ,\n",
       "       -2.78139997, -0.15366   ,  0.01015   ,  1.17980003,  0.15167999,\n",
       "       -0.050112  ,  1.26259995, -0.77526999,  0.36030999,  0.95761001,\n",
       "       -0.11385   ,  0.28035   , -0.02591   ,  0.31246001, -0.15424   ,\n",
       "        0.37779999, -0.13598999,  0.29460001, -0.31579   ,  0.42943001,\n",
       "        0.086969  ,  0.019169  , -0.27241999, -0.31696001,  0.37327   ,\n",
       "        0.61997002,  0.13889   ,  0.17188001,  0.30362999, -1.27760005,\n",
       "        0.044423  , -0.52736002, -0.88536   , -0.19428   , -0.61947   ,\n",
       "       -0.10146   , -0.26301   , -0.061707  ,  0.36627001, -0.95222998,\n",
       "       -0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibFXLQvXpjd0"
   },
   "source": [
    "이전에 확인한 사전에 훈련된 GloVe에서의 'great'의 벡터값과 일치합니다. 이제 Embedding layer에 embedding_matrix를 초기값으로 설정합니다. 현재 실습에서 사전 훈련된 워드 임베딩을 100차원의 값인 것으로 사용하고 있기 때문에 임베딩 층의 output_dim의 인자값으로 100을 주어야 합니다. 그리고 사전 훈련된 워드 임베딩을 그대로 사용할 경우 추가 훈련을 하지 않는다는 의미에서 trainable의 인자값을 False로 선택할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUBXRpt2Qzwn"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irJTyKCWQ1L3",
    "outputId": "e5a2f0ed-4064-44ee-b160-5b30b5eed173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 0s - loss: 0.7021 - acc: 0.5714\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.6829 - acc: 0.5714\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.6644 - acc: 0.5714\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.6466 - acc: 0.5714\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.6294 - acc: 0.5714\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.6129 - acc: 0.5714\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.5969 - acc: 0.7143\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.5816 - acc: 0.7143\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.5668 - acc: 0.7143\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.5525 - acc: 0.7143\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.5386 - acc: 0.7143\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.5253 - acc: 0.7143\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.5124 - acc: 0.7143\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 0.4999 - acc: 0.8571\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 0.4878 - acc: 0.8571\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 0.4760 - acc: 0.8571\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 0.4647 - acc: 0.8571\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 0.4536 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 0.4429 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 0.4325 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 0.4224 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 0.4125 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 0.4030 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 0.3937 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 0.3847 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 0.3759 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 0.3674 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 0.3591 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 0.3510 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 0.3432 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 0.3355 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 0.3281 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 0.3209 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 0.3139 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 0.3071 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 0.3005 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 0.2941 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 0.2879 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 0.2818 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 0.2759 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 0.2702 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 0.2646 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 0.2592 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 0.2540 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 0.2489 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 0.2439 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 0.2391 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 0.2344 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 0.2298 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 0.2254 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 0.2211 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 0.2169 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 0.2128 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 0.2089 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 0.2050 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 0.2012 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 0.1976 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 0.1940 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 0.1906 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 0.1872 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 0.1839 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 0.1807 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 0.1776 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 0.1746 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 0.1716 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 0.1688 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 0.1660 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 0.1632 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 0.1606 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 0.1580 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 0.1554 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 0.1529 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 0.1505 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 0.1482 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 0.1459 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 0.1436 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 0.1415 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 0.1393 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 0.1372 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 0.1352 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 0.1332 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 0.1313 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 0.1294 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 0.1275 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 0.1257 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 0.1239 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 0.1222 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 0.1205 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 0.1188 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 0.1172 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 0.1156 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 0.1140 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 0.1125 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 0.1110 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 0.1096 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 0.1082 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 0.1068 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 0.1054 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 0.1040 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 0.1027 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff42dace850>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yU9xJhvpsMf"
   },
   "source": [
    "사전 훈련된 GloVe 임베딩에 대한 예제는 아래의 케라스 블로그 링크에도 기재되어 있습니다.  \n",
    "  \n",
    "* 링크 : https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T1O4SrARZzI"
   },
   "source": [
    "#### **2) 사전 훈련된 Word2Vec 사용하기**\n",
    "구글의 사전 훈련된 Word2Vec 모델을 로드하여 word2vec_model에 저장 후 크기를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFhk9EtKRkVG"
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6JqBwQtQ3q3"
   },
   "outputs": [],
   "source": [
    "urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n",
    "                           filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZrpBwRoRkhf",
    "outputId": "801a833a-ddd8-4511-b954-26849253e689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model.vectors.shape) # 모델의 크기 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c_rfb-jp447"
   },
   "source": [
    "300의 차원을 가진 Word2Vec 벡터가 3,000,000개 있습니다. 모든 값이 0으로 채워진 임베딩 행렬을 만들어줍니다. 풀고자 하는 문제의 단어 집합 크기의 행과 300개의 열을 가지는 행렬 생성합니다. 이 행렬의 값은 전부 0으로 채웁니다. 이 행렬에 사전 훈련된 임베딩 값을 넣어줄 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwUO0-ewRljP",
    "outputId": "ece13966-cf15-4afd-a4ea-5636e7bc2387"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 집합 크기의 행과 300개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPnJH3a3p_uL"
   },
   "source": [
    "word2vec_model에서 특정 단어를 입력하면 해당 단어의 임베딩 벡터를 리턴받을텐데, 만약 word2vec_model에 특정 단어의 임베딩 벡터가 없다면 None을 리턴하도록 하는 함수 get_vector()를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmScXGKyRm7n"
   },
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciI4ESyfqFM2"
   },
   "source": [
    "단어 집합으로부터 단어를 1개씩 호출하여 word2vec_model에 해당 단어의 임베딩 벡터값이 존재하는지 확인합니다. 만약 None이 아니라면 존재한다는 의미이므로 임베딩 행렬에 해당 단어의 인덱스 위치의 행에 임베딩 벡터의 값을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyUpH9gbRnv3"
   },
   "outputs": [],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
    "    vector_value = get_vector(word)\n",
    "    if vector_value is not None:\n",
    "        embedding_matrix[index] = vector_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOqvtK3OqHHJ"
   },
   "source": [
    "현재 풀고자하는 문제의 16개의 단어와 맵핑되는 임베딩 행렬이 완성됩니다. 제대로 맵핑이 됐는지 확인해볼까요? 기존에 word2vec_model에 저장되어 있던 단어 'nice'의 임베딩 벡터값을 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wviBQBZRswP",
    "outputId": "6a29ac76-cc12-4849-a193-ace1984c5c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\n",
      "  0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\n",
      " -0.01806641 -0.25390625  0.13183594  0.0859375   0.16113281  0.11083984\n",
      " -0.11083984 -0.0859375   0.0267334   0.34570312  0.15136719 -0.00415039\n",
      "  0.10498047  0.04907227 -0.06982422  0.08642578  0.03198242 -0.02844238\n",
      " -0.15722656  0.11865234  0.36132812  0.00173187  0.05297852 -0.234375\n",
      "  0.11767578  0.08642578 -0.01123047  0.25976562  0.28515625 -0.11669922\n",
      "  0.38476562  0.07275391  0.01147461  0.03466797  0.18164062 -0.03955078\n",
      "  0.04199219  0.01013184 -0.06054688  0.09765625  0.06689453  0.14648438\n",
      " -0.12011719  0.08447266 -0.06152344  0.06347656  0.3046875  -0.35546875\n",
      " -0.2890625   0.19628906 -0.33203125 -0.07128906  0.12792969  0.09619141\n",
      " -0.12158203 -0.08691406 -0.12890625  0.27734375  0.265625    0.1796875\n",
      "  0.12695312  0.06298828 -0.34375    -0.05908203  0.0456543   0.171875\n",
      "  0.08935547  0.14648438 -0.04638672 -0.00842285 -0.0279541   0.234375\n",
      " -0.07470703 -0.13574219  0.00378418  0.19433594  0.05664062 -0.05419922\n",
      "  0.06176758  0.14160156 -0.24121094  0.02539062 -0.15917969 -0.10595703\n",
      "  0.11865234  0.24707031 -0.13574219 -0.20410156 -0.30078125  0.07910156\n",
      " -0.04394531  0.02026367 -0.05786133  0.2109375   0.13574219  0.08349609\n",
      " -0.0098877  -0.10546875 -0.08105469  0.03735352 -0.10351562 -0.10205078\n",
      "  0.23925781 -0.21875     0.05151367  0.06738281  0.07617188  0.04638672\n",
      "  0.03198242 -0.07275391  0.14550781  0.04858398 -0.05664062 -0.07470703\n",
      " -0.0030365  -0.09277344 -0.11083984 -0.03320312 -0.15234375 -0.12207031\n",
      "  0.09814453  0.375       0.00454712 -0.10009766  0.02734375  0.30078125\n",
      " -0.0390625   0.30078125 -0.04541016 -0.00424194  0.13671875 -0.18945312\n",
      " -0.21777344  0.12695312 -0.02746582 -0.18164062  0.08984375 -0.23339844\n",
      "  0.203125    0.2734375  -0.26953125  0.15332031 -0.20703125 -0.01153564\n",
      "  0.12451172  0.05395508 -0.23535156 -0.01409912 -0.09765625  0.20800781\n",
      "  0.19335938  0.14746094  0.28710938 -0.23046875  0.01965332 -0.09619141\n",
      " -0.0703125  -0.04174805 -0.17578125  0.0007019   0.10546875  0.10351562\n",
      "  0.02478027  0.35742188  0.17382812 -0.09570312 -0.18359375  0.23242188\n",
      " -0.14453125 -0.20410156 -0.01867676  0.06640625 -0.2265625  -0.00582886\n",
      " -0.08642578  0.02416992 -0.07324219 -0.29882812 -0.15625     0.07666016\n",
      "  0.19628906 -0.20410156  0.09863281 -0.01672363 -0.18652344 -0.12353516\n",
      " -0.16015625 -0.10058594  0.21777344  0.09375    -0.10058594 -0.03637695\n",
      "  0.15136719 -0.02526855 -0.23730469  0.03417969 -0.00604248  0.15625\n",
      " -0.14257812  0.18066406 -0.35351562  0.25        0.13085938 -0.04296875\n",
      "  0.17089844  0.20507812  0.00680542 -0.08251953 -0.06738281  0.22167969\n",
      " -0.16308594 -0.16699219 -0.02087402  0.11035156  0.06054688 -0.04223633\n",
      " -0.17285156  0.05029297 -0.19824219  0.01495361  0.06542969  0.03271484\n",
      "  0.14453125 -0.08691406 -0.11035156 -0.1484375   0.09667969  0.22363281\n",
      "  0.23535156  0.08398438  0.18164062 -0.10595703 -0.04296875  0.11572266\n",
      " -0.00153351  0.0534668  -0.1328125  -0.33203125 -0.08251953  0.30664062\n",
      "  0.22363281  0.27929688  0.09082031 -0.18066406 -0.00613403 -0.09423828\n",
      " -0.21289062  0.01965332 -0.08105469 -0.06689453 -0.31835938 -0.08447266\n",
      "  0.13574219  0.0625      0.07080078 -0.14257812 -0.11279297  0.01452637\n",
      " -0.06689453  0.03881836  0.19433594  0.09521484  0.11376953 -0.12451172\n",
      "  0.13769531 -0.18847656 -0.05224609  0.15820312  0.09863281 -0.04370117\n",
      " -0.06054688  0.21679688  0.04077148 -0.14648438 -0.18945312 -0.25195312\n",
      " -0.16894531 -0.08642578 -0.08544922  0.18945312 -0.14648438  0.13476562\n",
      " -0.04077148  0.03271484  0.08935547 -0.26757812  0.00836182 -0.21386719]\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model['nice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcyO-wqZqQpD"
   },
   "source": [
    "단어 'nice'의 맵핑된 정수를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kx8jztkgqZTo"
   },
   "outputs": [],
   "source": [
    "print('단어 nice의 맵핑된 정수 :', tokenizer.word_index['nice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSo-1NFwqga7"
   },
   "source": [
    "1의 값을 가지므로 embedding_matirx의 1번 인덱스에는 단어 'nice'의 임베딩 벡터값이 있어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtCa09MKqoOx"
   },
   "source": [
    "값이 word2vec_model에서 확인했던 것과 동일한 것을 확인할 수 있습니다. 단어 집합에 있는 다른 단어들에 대해서도 확인해보세요. 이제 Embedding에 사전 훈련된 embedding_matrix를 입력으로 넣어주고 모델을 학습시켜보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-s4iYh0qq6e"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len,), dtype='int32'))\n",
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RnlsBWzqtZz"
   },
   "source": [
    "사전 훈련된 워드 임베딩을 이용한 텍스트 분류는 'NLP를 이용한 합성곱 신경망' 챕터의 의도 분류 실습( https://wikidocs.net/86083 )을 참고하세요.\n",
    "\n",
    "마지막 편집일시 : 2022년 11월 14일 2:59 오후"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X36vBKkLqxTU"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7FAtTr-qxPp"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1fYqXfSqi2f"
   },
   "outputs": [],
   "source": [
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCf1yEg6Rtk3",
    "outputId": "3c685473-ce25-4a80-fb13-4d2568dbfaf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.17773438e-02  2.08007812e-01 -2.84423828e-02  1.78710938e-01\n",
      "  1.32812500e-01 -9.96093750e-02  9.61914062e-02 -1.16699219e-01\n",
      " -8.54492188e-03  1.48437500e-01 -3.34472656e-02 -1.85546875e-01\n",
      "  4.10156250e-02 -8.98437500e-02  2.17285156e-02  6.93359375e-02\n",
      "  1.80664062e-01  2.22656250e-01 -1.00585938e-01 -6.93359375e-02\n",
      "  1.04427338e-04  1.60156250e-01  4.07714844e-02  7.37304688e-02\n",
      "  1.53320312e-01  6.78710938e-02 -1.03027344e-01  4.17480469e-02\n",
      "  4.27246094e-02 -1.10351562e-01 -6.68945312e-02  4.19921875e-02\n",
      "  2.50000000e-01  2.12890625e-01  1.59179688e-01  1.44653320e-02\n",
      " -4.88281250e-02  1.39770508e-02  3.55529785e-03  2.09960938e-01\n",
      "  1.52343750e-01 -7.32421875e-02  2.16796875e-01 -5.76171875e-02\n",
      " -2.84423828e-02 -3.60107422e-03  1.52343750e-01 -2.63671875e-02\n",
      "  2.13623047e-02 -1.51367188e-01  1.04003906e-01  3.18359375e-01\n",
      " -1.85546875e-01  3.68652344e-02 -1.10839844e-01 -3.17382812e-02\n",
      " -1.01562500e-01 -1.21093750e-01  3.22265625e-01 -7.32421875e-02\n",
      " -1.52343750e-01  2.67578125e-01 -1.50390625e-01 -1.23046875e-01\n",
      "  1.07910156e-01  6.68945312e-02 -2.13623047e-02 -1.00585938e-01\n",
      " -2.05078125e-01  1.17675781e-01  6.15234375e-02  6.78710938e-02\n",
      "  1.06933594e-01 -7.71484375e-02 -1.52343750e-01 -4.24194336e-03\n",
      " -1.45507812e-01  2.53906250e-01  4.80957031e-02  9.71679688e-02\n",
      " -8.36181641e-03  1.12792969e-01  5.34667969e-02  1.79443359e-02\n",
      " -5.63964844e-02 -3.30078125e-01 -9.76562500e-02  1.42578125e-01\n",
      " -1.37695312e-01  2.20947266e-02  1.00097656e-01 -5.71289062e-02\n",
      " -1.56250000e-01 -6.37817383e-03 -9.37500000e-02 -4.68750000e-02\n",
      "  8.59375000e-02  3.06640625e-01 -1.11328125e-01 -1.94335938e-01\n",
      " -2.08007812e-01  8.10546875e-02 -4.19921875e-02 -8.30078125e-02\n",
      " -1.04003906e-01  2.92968750e-01  2.39257812e-02 -3.85742188e-02\n",
      "  3.56445312e-02 -1.04980469e-01 -6.54296875e-02  2.79296875e-01\n",
      " -1.16210938e-01 -1.45874023e-02  3.84765625e-01 -7.81250000e-02\n",
      " -2.92968750e-02 -1.35742188e-01 -5.39550781e-02 -5.49316406e-02\n",
      " -8.10546875e-02 -2.88085938e-02  8.34960938e-02  2.73437500e-01\n",
      " -6.20117188e-02 -4.78515625e-02 -1.09252930e-02 -1.13769531e-01\n",
      " -1.09863281e-01  2.02148438e-01 -1.28906250e-01 -6.68945312e-02\n",
      " -2.67578125e-01  9.61914062e-02  1.04003906e-01 -1.69921875e-01\n",
      "  5.56640625e-02  1.54296875e-01  8.05664062e-02  2.19726562e-01\n",
      " -2.27539062e-01  1.10351562e-01 -8.11767578e-03 -5.63964844e-02\n",
      " -9.03320312e-02 -7.76367188e-02 -3.61328125e-02  3.61328125e-02\n",
      "  1.58203125e-01 -1.56250000e-01  2.26562500e-01  2.85156250e-01\n",
      " -5.51757812e-02  3.53515625e-01 -1.20605469e-01  1.05957031e-01\n",
      "  3.11279297e-02 -1.91406250e-01 -2.31445312e-01 -1.11816406e-01\n",
      "  2.38037109e-03  7.51953125e-02 -1.28784180e-02  1.00585938e-01\n",
      "  4.45312500e-01 -2.77343750e-01  6.68945312e-02 -8.10546875e-02\n",
      "  6.39648438e-02  1.85546875e-02 -1.11328125e-01  9.76562500e-02\n",
      "  2.06054688e-01 -1.30859375e-01  2.39257812e-02  1.10839844e-01\n",
      "  8.05664062e-02 -1.52343750e-01  4.85229492e-03  1.84326172e-02\n",
      " -9.17968750e-02 -2.41210938e-01  8.39843750e-02 -1.00585938e-01\n",
      " -1.54296875e-01  2.75878906e-02 -1.64062500e-01 -1.01562500e-01\n",
      " -6.07299805e-03  1.33514404e-03 -2.53906250e-01  3.14453125e-01\n",
      "  1.31835938e-01 -1.31835938e-01  2.17285156e-02 -1.56250000e-01\n",
      " -1.46484375e-01 -5.12695312e-02 -1.20605469e-01 -2.15820312e-01\n",
      "  3.10058594e-02  1.30859375e-01  9.71679688e-02  5.67626953e-03\n",
      "  2.20947266e-02  1.26953125e-01 -1.24511719e-02  6.15234375e-02\n",
      " -2.23388672e-02  2.50000000e-01 -7.17773438e-02  1.58203125e-01\n",
      " -7.27539062e-02  1.97753906e-02  8.85009766e-03 -9.08203125e-02\n",
      "  3.63281250e-01 -9.03320312e-02  2.41699219e-02 -1.39770508e-02\n",
      " -5.10253906e-02  2.40478516e-02  5.88989258e-03 -1.02050781e-01\n",
      " -8.85009766e-03  3.05175781e-02 -7.81250000e-02 -1.27929688e-01\n",
      "  3.85742188e-02  2.86865234e-02 -2.28515625e-01 -1.25122070e-02\n",
      "  1.54296875e-01  9.13085938e-02  1.05468750e-01 -6.44531250e-02\n",
      " -1.28906250e-01 -1.02050781e-01 -2.16064453e-02 -3.29589844e-02\n",
      "  7.47070312e-02  3.78417969e-02  7.42187500e-02 -1.23901367e-02\n",
      " -4.68750000e-02  4.88281250e-03  1.03515625e-01 -8.69140625e-02\n",
      " -2.26562500e-01 -2.53906250e-01  3.58886719e-02  4.45312500e-01\n",
      "  5.56640625e-02  1.59179688e-01  2.71484375e-01 -1.08398438e-01\n",
      "  6.25000000e-02 -5.59082031e-02 -2.50000000e-01 -1.55273438e-01\n",
      " -6.83593750e-02 -1.39648438e-01 -1.59179688e-01 -1.79443359e-02\n",
      "  2.12402344e-02  7.37304688e-02  1.30859375e-01 -8.05664062e-02\n",
      "  2.99072266e-02  1.55639648e-02 -1.66015625e-01  1.50390625e-01\n",
      " -6.77490234e-03  1.01318359e-02  1.14746094e-01 -1.48437500e-01\n",
      " -4.58984375e-02 -1.39648438e-01 -1.73828125e-01 -4.27246094e-02\n",
      " -5.81054688e-02  5.22460938e-02 -1.11328125e-01  8.44726562e-02\n",
      " -2.55126953e-02  1.40625000e-01 -1.81640625e-01  1.72119141e-02\n",
      " -1.37695312e-01 -1.47705078e-02 -1.14746094e-02  6.44531250e-02\n",
      " -2.89062500e-01 -4.80957031e-02 -1.99218750e-01 -7.12890625e-02\n",
      "  6.44531250e-02 -1.67968750e-01 -2.08740234e-02 -1.42578125e-01]\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model['great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-xnWKEZ7RvZ3",
    "outputId": "24abcda0-f864-4d64-8a3a-83592c19c93b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.17773438e-02  2.08007812e-01 -2.84423828e-02  1.78710938e-01\n",
      "  1.32812500e-01 -9.96093750e-02  9.61914062e-02 -1.16699219e-01\n",
      " -8.54492188e-03  1.48437500e-01 -3.34472656e-02 -1.85546875e-01\n",
      "  4.10156250e-02 -8.98437500e-02  2.17285156e-02  6.93359375e-02\n",
      "  1.80664062e-01  2.22656250e-01 -1.00585938e-01 -6.93359375e-02\n",
      "  1.04427338e-04  1.60156250e-01  4.07714844e-02  7.37304688e-02\n",
      "  1.53320312e-01  6.78710938e-02 -1.03027344e-01  4.17480469e-02\n",
      "  4.27246094e-02 -1.10351562e-01 -6.68945312e-02  4.19921875e-02\n",
      "  2.50000000e-01  2.12890625e-01  1.59179688e-01  1.44653320e-02\n",
      " -4.88281250e-02  1.39770508e-02  3.55529785e-03  2.09960938e-01\n",
      "  1.52343750e-01 -7.32421875e-02  2.16796875e-01 -5.76171875e-02\n",
      " -2.84423828e-02 -3.60107422e-03  1.52343750e-01 -2.63671875e-02\n",
      "  2.13623047e-02 -1.51367188e-01  1.04003906e-01  3.18359375e-01\n",
      " -1.85546875e-01  3.68652344e-02 -1.10839844e-01 -3.17382812e-02\n",
      " -1.01562500e-01 -1.21093750e-01  3.22265625e-01 -7.32421875e-02\n",
      " -1.52343750e-01  2.67578125e-01 -1.50390625e-01 -1.23046875e-01\n",
      "  1.07910156e-01  6.68945312e-02 -2.13623047e-02 -1.00585938e-01\n",
      " -2.05078125e-01  1.17675781e-01  6.15234375e-02  6.78710938e-02\n",
      "  1.06933594e-01 -7.71484375e-02 -1.52343750e-01 -4.24194336e-03\n",
      " -1.45507812e-01  2.53906250e-01  4.80957031e-02  9.71679688e-02\n",
      " -8.36181641e-03  1.12792969e-01  5.34667969e-02  1.79443359e-02\n",
      " -5.63964844e-02 -3.30078125e-01 -9.76562500e-02  1.42578125e-01\n",
      " -1.37695312e-01  2.20947266e-02  1.00097656e-01 -5.71289062e-02\n",
      " -1.56250000e-01 -6.37817383e-03 -9.37500000e-02 -4.68750000e-02\n",
      "  8.59375000e-02  3.06640625e-01 -1.11328125e-01 -1.94335938e-01\n",
      " -2.08007812e-01  8.10546875e-02 -4.19921875e-02 -8.30078125e-02\n",
      " -1.04003906e-01  2.92968750e-01  2.39257812e-02 -3.85742188e-02\n",
      "  3.56445312e-02 -1.04980469e-01 -6.54296875e-02  2.79296875e-01\n",
      " -1.16210938e-01 -1.45874023e-02  3.84765625e-01 -7.81250000e-02\n",
      " -2.92968750e-02 -1.35742188e-01 -5.39550781e-02 -5.49316406e-02\n",
      " -8.10546875e-02 -2.88085938e-02  8.34960938e-02  2.73437500e-01\n",
      " -6.20117188e-02 -4.78515625e-02 -1.09252930e-02 -1.13769531e-01\n",
      " -1.09863281e-01  2.02148438e-01 -1.28906250e-01 -6.68945312e-02\n",
      " -2.67578125e-01  9.61914062e-02  1.04003906e-01 -1.69921875e-01\n",
      "  5.56640625e-02  1.54296875e-01  8.05664062e-02  2.19726562e-01\n",
      " -2.27539062e-01  1.10351562e-01 -8.11767578e-03 -5.63964844e-02\n",
      " -9.03320312e-02 -7.76367188e-02 -3.61328125e-02  3.61328125e-02\n",
      "  1.58203125e-01 -1.56250000e-01  2.26562500e-01  2.85156250e-01\n",
      " -5.51757812e-02  3.53515625e-01 -1.20605469e-01  1.05957031e-01\n",
      "  3.11279297e-02 -1.91406250e-01 -2.31445312e-01 -1.11816406e-01\n",
      "  2.38037109e-03  7.51953125e-02 -1.28784180e-02  1.00585938e-01\n",
      "  4.45312500e-01 -2.77343750e-01  6.68945312e-02 -8.10546875e-02\n",
      "  6.39648438e-02  1.85546875e-02 -1.11328125e-01  9.76562500e-02\n",
      "  2.06054688e-01 -1.30859375e-01  2.39257812e-02  1.10839844e-01\n",
      "  8.05664062e-02 -1.52343750e-01  4.85229492e-03  1.84326172e-02\n",
      " -9.17968750e-02 -2.41210938e-01  8.39843750e-02 -1.00585938e-01\n",
      " -1.54296875e-01  2.75878906e-02 -1.64062500e-01 -1.01562500e-01\n",
      " -6.07299805e-03  1.33514404e-03 -2.53906250e-01  3.14453125e-01\n",
      "  1.31835938e-01 -1.31835938e-01  2.17285156e-02 -1.56250000e-01\n",
      " -1.46484375e-01 -5.12695312e-02 -1.20605469e-01 -2.15820312e-01\n",
      "  3.10058594e-02  1.30859375e-01  9.71679688e-02  5.67626953e-03\n",
      "  2.20947266e-02  1.26953125e-01 -1.24511719e-02  6.15234375e-02\n",
      " -2.23388672e-02  2.50000000e-01 -7.17773438e-02  1.58203125e-01\n",
      " -7.27539062e-02  1.97753906e-02  8.85009766e-03 -9.08203125e-02\n",
      "  3.63281250e-01 -9.03320312e-02  2.41699219e-02 -1.39770508e-02\n",
      " -5.10253906e-02  2.40478516e-02  5.88989258e-03 -1.02050781e-01\n",
      " -8.85009766e-03  3.05175781e-02 -7.81250000e-02 -1.27929688e-01\n",
      "  3.85742188e-02  2.86865234e-02 -2.28515625e-01 -1.25122070e-02\n",
      "  1.54296875e-01  9.13085938e-02  1.05468750e-01 -6.44531250e-02\n",
      " -1.28906250e-01 -1.02050781e-01 -2.16064453e-02 -3.29589844e-02\n",
      "  7.47070312e-02  3.78417969e-02  7.42187500e-02 -1.23901367e-02\n",
      " -4.68750000e-02  4.88281250e-03  1.03515625e-01 -8.69140625e-02\n",
      " -2.26562500e-01 -2.53906250e-01  3.58886719e-02  4.45312500e-01\n",
      "  5.56640625e-02  1.59179688e-01  2.71484375e-01 -1.08398438e-01\n",
      "  6.25000000e-02 -5.59082031e-02 -2.50000000e-01 -1.55273438e-01\n",
      " -6.83593750e-02 -1.39648438e-01 -1.59179688e-01 -1.79443359e-02\n",
      "  2.12402344e-02  7.37304688e-02  1.30859375e-01 -8.05664062e-02\n",
      "  2.99072266e-02  1.55639648e-02 -1.66015625e-01  1.50390625e-01\n",
      " -6.77490234e-03  1.01318359e-02  1.14746094e-01 -1.48437500e-01\n",
      " -4.58984375e-02 -1.39648438e-01 -1.73828125e-01 -4.27246094e-02\n",
      " -5.81054688e-02  5.22460938e-02 -1.11328125e-01  8.44726562e-02\n",
      " -2.55126953e-02  1.40625000e-01 -1.81640625e-01  1.72119141e-02\n",
      " -1.37695312e-01 -1.47705078e-02 -1.14746094e-02  6.44531250e-02\n",
      " -2.89062500e-01 -4.80957031e-02 -1.99218750e-01 -7.12890625e-02\n",
      "  6.44531250e-02 -1.67968750e-01 -2.08740234e-02 -1.42578125e-01]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJmqMQ69RwfY",
    "outputId": "ba630b73-d8e7-4dde-ea9b-10d417154f91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 0s - loss: 0.7373 - acc: 0.4286\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.7183 - acc: 0.4286\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.6998 - acc: 0.7143\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.6819 - acc: 0.7143\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.6644 - acc: 0.7143\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.6475 - acc: 0.8571\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.6312 - acc: 0.8571\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.6153 - acc: 0.8571\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.5999 - acc: 0.8571\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.5850 - acc: 0.8571\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.5706 - acc: 0.8571\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.5566 - acc: 0.8571\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.5431 - acc: 0.8571\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 0.5300 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 0.5174 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 0.5051 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 0.4932 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 0.4817 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 0.4705 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 0.4597 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 0.4492 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 0.4390 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 0.4291 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 0.4195 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 0.4102 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 0.4012 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 0.3925 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 0.3840 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 0.3757 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 0.3677 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 0.3600 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 0.3524 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 0.3451 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 0.3380 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 0.3311 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 0.3244 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 0.3179 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 0.3115 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 0.3054 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 0.2994 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 0.2936 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 0.2879 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 0.2824 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 0.2771 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 0.2719 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 0.2668 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 0.2619 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 0.2571 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 0.2525 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 0.2479 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 0.2435 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 0.2392 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 0.2350 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 0.2310 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 0.2270 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 0.2231 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 0.2194 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 0.2157 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 0.2121 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 0.2086 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 0.2052 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 0.2019 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 0.1987 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 0.1955 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 0.1924 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 0.1894 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 0.1865 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 0.1836 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 0.1808 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 0.1781 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 0.1754 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 0.1728 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 0.1702 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 0.1677 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 0.1653 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 0.1629 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 0.1606 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 0.1583 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 0.1561 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 0.1539 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 0.1518 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 0.1497 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 0.1477 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 0.1457 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 0.1437 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 0.1418 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 0.1399 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 0.1381 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 0.1363 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 0.1345 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 0.1328 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 0.1311 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 0.1294 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 0.1278 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 0.1262 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 0.1247 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 0.1231 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 0.1216 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 0.1201 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 0.1187 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff3e73e8210>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len,), dtype='int32'))\n",
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py310_yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
