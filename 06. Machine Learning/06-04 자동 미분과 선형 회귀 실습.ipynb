{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GBW64BCOofy"
      },
      "source": [
        "이 자료는 위키독스 딥 러닝을 이용한 자연어 처리 입문의 자동 미분과 선형 회귀 튜토리얼입니다.  \n",
        "\n",
        "링크 : https://wikidocs.net/111472"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **06-04 자동 미분과 선형 회귀 실습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUW8bf-sSkbO"
      },
      "source": [
        "### **1. 자동 미분**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p6FWvMAPNM73"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "gXnpYeOBNPst",
        "outputId": "e9c59450-6d3e-414a-b519-721bc894af04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.19.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "tape_gradient()는 자동 미분 기능을 수행합니다. 임의로 <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
        "  <mn>2</mn>\n",
        "  <msup>\n",
        "    <mi>w</mi>\n",
        "    <mn>2</mn>\n",
        "  </msup>\n",
        "  <mo>+</mo>\n",
        "  <mn>5</mn>\n",
        "</math>라는 식을 세워보고, 에 대해 미분해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BNnzmU5WNQyk"
      },
      "outputs": [],
      "source": [
        "w = tf.Variable(2.)\n",
        "\n",
        "def f(w):\n",
        "  y = w**2\n",
        "  z = 2*y + 5\n",
        "  return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "gradients를 출력하면 <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
        "  <mi>w</mi>\n",
        "</math>\n",
        "에 대해 미분한 값이 저장된 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51Y6C6xfNR_s",
        "outputId": "3d56532b-654d-4fb8-ac0d-f7734bdc97d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>]\n"
          ]
        }
      ],
      "source": [
        "with tf.GradientTape() as tape:\n",
        "  z = f(w)\n",
        "\n",
        "gradients = tape.gradient(z, [w])\n",
        "print(gradients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8.0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gradients[0].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 자동 미분 기능을 사용하여 선형 회귀를 구현해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRwwF03lSoBy"
      },
      "source": [
        "### **2. 자동 미분을 이용한 선형 회귀 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "우선 가중치 변수 w와 b를 선언합니다. 학습될 값이므로 임의의 값인 4와 1로 초기화하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zK0aP09pNStk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.0>,\n",
              " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 학습될 가중치 변수를 선언\n",
        "W = tf.Variable(4.0)\n",
        "b = tf.Variable(1.0)\n",
        "W, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "가설을 함수로서 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hV4Z5l3yNTfs"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def hypothesis(x):\n",
        "  return W*x + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### @tf.function의 역할\n",
        "\n",
        "**개요**\n",
        "  \n",
        "* `@tf.function`은 TensorFlow에서 **Python 함수를 고성능 그래프 함수(Graph Function)** 로 변환해주는 데코레이터  \n",
        "* 이를 통해 **성능 향상**, **GPU/TPU 연산 최적화**, **자동 최적화**가 가능해집니다.\n",
        "\n",
        "\n",
        "**주요 역할**\n",
        "\n",
        "| 기능              | 설명 |\n",
        "|-------------------|------|\n",
        "| **그래프 변환**    | Python 코드를 TensorFlow 계산 그래프로 변환 |\n",
        "| **속도 향상**      | Python 인터프리터 없이 빠르게 실행 |\n",
        "| **자동 최적화**    | 불필요한 연산 병합, 계산 캐싱 등 최적화 |\n",
        "| **분산 실행 가능** | GPU, TPU에서 실행 가능 |\n",
        "| **Autograph 지원** | `if`, `for`, `while` 문도 그래프화 가능 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "현재의 가설에서 w와 b는 각각 4와 1이므로 임의의 입력값을 넣었을 때의 결과는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wskrej1QNUP0",
        "outputId": "3a30d9c8-5779-48f1-f0ad-6f31fe66b089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15. 21. 23. 25.]\n"
          ]
        }
      ],
      "source": [
        "x_test = [3.5, 5, 5.5, 6]\n",
        "print(hypothesis(x_test).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([15., 21., 23., 25.], dtype=float32)>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hypothesis(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([15., 21., 23., 25.], dtype=float32)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hypothesis(x_test).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "다음과 같이 평균 제곱 오차를 손실 함수로서 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8xmmgZ90NVIr"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def mse_loss(y_pred, y):\n",
        "  # 두 개의 차이값을 제곱을 해서 평균을 취한다.\n",
        "  return tf.reduce_mean(tf.square(y_pred - y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "여기서 사용할 데이터는 x와 y가 약 10배의 차이를 가지는 데이터입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qRI9TWU-NWcE"
      },
      "outputs": [],
      "source": [
        "x = [1, 2, 3, 4, 5, 6, 7, 8, 9] # 공부하는 시간\n",
        "y = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "옵티마이저는 경사 하강법을 사용하되, 학습률(learning rate)는 0.01을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YTsCMtv4NbCT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.src.optimizers.sgd.SGD at 0x230d4d0b820>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer = tf.optimizers.SGD(0.01)\n",
        "optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "약 300번에 걸쳐서 경사 하강법을 수행하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxeO-C8mNb7T",
        "outputId": "e39711e5-affc-43e6-cf81-53ab01fe6db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch :   0 | W의 값 : 10.6671 | b의 값 : 0.908 | cost : 1.062102\n",
            "epoch :  10 | W의 값 : 10.6677 | b의 값 : 0.9039 | cost : 1.061931\n",
            "epoch :  20 | W의 값 : 10.6683 | b의 값 : 0.8999 | cost : 1.061765\n",
            "epoch :  30 | W의 값 : 10.6689 | b의 값 : 0.8961 | cost : 1.061616\n",
            "epoch :  40 | W의 값 : 10.6695 | b의 값 : 0.8924 | cost : 1.061478\n",
            "epoch :  50 | W의 값 : 10.6701 | b의 값 : 0.8889 | cost : 1.061354\n",
            "epoch :  60 | W의 값 : 10.6706 | b의 값 : 0.8856 | cost : 1.061237\n",
            "epoch :  70 | W의 값 : 10.6711 | b의 값 : 0.8824 | cost : 1.061131\n",
            "epoch :  80 | W의 값 : 10.6716 | b의 값 : 0.8793 | cost : 1.061027\n",
            "epoch :  90 | W의 값 : 10.6721 | b의 값 : 0.8763 | cost : 1.060937\n",
            "epoch : 100 | W의 값 : 10.6725 | b의 값 : 0.8734 | cost : 1.060852\n",
            "epoch : 110 | W의 값 : 10.6730 | b의 값 : 0.8707 | cost : 1.060777\n",
            "epoch : 120 | W의 값 : 10.6734 | b의 값 : 0.8681 | cost : 1.060702\n",
            "epoch : 130 | W의 값 : 10.6738 | b의 값 : 0.8655 | cost : 1.060640\n",
            "epoch : 140 | W의 값 : 10.6742 | b의 값 : 0.8631 | cost : 1.060583\n",
            "epoch : 150 | W의 값 : 10.6746 | b의 값 : 0.8608 | cost : 1.060523\n",
            "epoch : 160 | W의 값 : 10.6749 | b의 값 : 0.8586 | cost : 1.060475\n",
            "epoch : 170 | W의 값 : 10.6752 | b의 값 : 0.8564 | cost : 1.060429\n",
            "epoch : 180 | W의 값 : 10.6756 | b의 값 : 0.8544 | cost : 1.060382\n",
            "epoch : 190 | W의 값 : 10.6759 | b의 값 : 0.8524 | cost : 1.060344\n",
            "epoch : 200 | W의 값 : 10.6762 | b의 값 : 0.8505 | cost : 1.060305\n",
            "epoch : 210 | W의 값 : 10.6765 | b의 값 : 0.8487 | cost : 1.060272\n",
            "epoch : 220 | W의 값 : 10.6768 | b의 값 : 0.847 | cost : 1.060245\n",
            "epoch : 230 | W의 값 : 10.6770 | b의 값 : 0.8453 | cost : 1.060210\n",
            "epoch : 240 | W의 값 : 10.6773 | b의 값 : 0.8437 | cost : 1.060186\n",
            "epoch : 250 | W의 값 : 10.6775 | b의 값 : 0.8422 | cost : 1.060160\n",
            "epoch : 260 | W의 값 : 10.6777 | b의 값 : 0.8407 | cost : 1.060138\n",
            "epoch : 270 | W의 값 : 10.6780 | b의 값 : 0.8393 | cost : 1.060120\n",
            "epoch : 280 | W의 값 : 10.6782 | b의 값 : 0.8379 | cost : 1.060100\n",
            "epoch : 290 | W의 값 : 10.6784 | b의 값 : 0.8366 | cost : 1.060077\n",
            "epoch : 300 | W의 값 : 10.6786 | b의 값 : 0.8354 | cost : 1.060062\n"
          ]
        }
      ],
      "source": [
        "for i in range(301):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # 현재 파라미터에 기반한 입력 x에 대한 예측값을 y_pred\n",
        "    y_pred = hypothesis(x)\n",
        "\n",
        "    # 평균 제곱 오차를 계산\n",
        "    cost = mse_loss(y_pred, y)\n",
        "\n",
        "  # 손실 함수에 대한 파라미터의 미분값 계산\n",
        "  gradients = tape.gradient(cost, [W, b])\n",
        "  \n",
        "  # 파라미터 업데이트\n",
        "  optimizer.apply_gradients(zip(gradients, [W, b]))\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    print(\"epoch : {:3} | W의 값 : {:5.4f} | b의 값 : {:5.4} | cost : {:5.6f}\".format(i, W.numpy(), b.numpy(), cost))\n",
        "  # break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "w와 b값이 계속 업데이트 됨에 따라서 cost가 지속적으로 줄어드는 것을 확인할 수 있습니다. 학습된 w와 b의 값에 대해서 임의 입력을 넣었을 경우의 예측값을 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0UZ5FPSNc1L",
        "outputId": "9e6d7d17-5b1e-40a8-96d4-bd2be505c35a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[38.21045  54.228344 59.567642 64.906944]\n"
          ]
        }
      ],
      "source": [
        "x_test = [3.5, 5, 5.5, 6]\n",
        "print(hypothesis(x_test).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "모델을 구현하는 방법은 한 가지가 아닙니다. 텐서플로우의 경우, 케라스라는 고수준의 API를 사용하면 모델을 이보다 좀 더 쉽게 구현할 수 있습니다. 이번에는 선형 회귀 모델을 케라스로 구현해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2x8w6WHSqh8"
      },
      "source": [
        "### **3. 케라스로 구현하는 선형 회귀**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "케라스에 대해서는 뒤의 딥 러닝 챕터에서 더 자세히 배우겠지만, 간단하게 케라스를 이용해서 선형 회귀를 구현해봅시다. 케라스로 모델을 만드는 기본적인 형식은 다음과 같습니다. Sequential로 model이라는 이름의 모델을 만들고, 그리고 add를 통해 입력과 출력 벡터의 차원과 같은 필요한 정보들을 추가해갑니다.\n",
        "\n",
        "아래의 예시 코드를 봅시다. 첫번째 인자인 1은 출력의 차원을 정의합니다. 일반적으로 output_dim으로 표현되는 인자입니다. 두번째 인자인 input_dim은 입력의 차원을 정의하는데 이번 실습과 같이 1개의 실수 <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
        "  <mi>x</mi>\n",
        "</math>\n",
        "를 가지고 하는 1개의 실수 <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
        "  <mi>y</mi>\n",
        "</math>\n",
        "를 예측하는 단순 선형 회귀를 구현하는 경우에는 각각 1의 값을 가집니다..\n",
        "\n",
        "```python\n",
        "    # 예시 코드. 실행 불가.\n",
        "    model = Sequential()\n",
        "    model.add(keras.layers.Dense(1, input_dim=1))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "실습을 진행해봅시다. 아래의 코드는 간단하지만, 지금까지 배운 것들이 집대성 된 코드입니다. 우선 공부한 시간을 <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
        "  <mi>x</mi>\n",
        "</math>\n",
        ", 각 공부한 시간에 따른 성적을 <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
        "  <mi>y</mi>\n",
        "</math>\n",
        "라고 해봅시다. activation은 어떤 함수를 사용할 것인지를 의미하는데 선형 회귀를 사용할 경우에는 linear라고 기재합니다.\n",
        "\n",
        "옵티마이저로 기본 경사 하강법을 사용하고 싶다면, sgd라고 기재합니다. 학습률은 0.01로 정하였습니다. 손실 함수로는 평균 제곱 오차를 사 용합니다. 그리고 전체 데이터에 대한 훈련 횟수는 300으로 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UarmrJMbNeIj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNqDWT86NfgD",
        "outputId": "44e97211-6b68-41dd-949c-7b1e684ff976"
      },
      "outputs": [],
      "source": [
        "x = [1, 2, 3, 4, 5, 6, 7, 8, 9] # 공부하는 시간\n",
        "y = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적\n",
        "import numpy as np\n",
        "x = np.array(x)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dlwlg\\anaconda3\\envs\\py310_yolo\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step - loss: 3185.1780 - mse: 3185.1780\n",
            "Epoch 2/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 392.9444 - mse: 392.9444\n",
            "Epoch 3/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 49.3801 - mse: 49.3801\n",
            "Epoch 4/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 7.1062 - mse: 7.1062\n",
            "Epoch 5/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.9039 - mse: 1.9039\n",
            "Epoch 6/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.2630 - mse: 1.2630\n",
            "Epoch 7/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.1833 - mse: 1.1833\n",
            "Epoch 8/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1727 - mse: 1.1727\n",
            "Epoch 9/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.1706 - mse: 1.1706\n",
            "Epoch 10/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.1696 - mse: 1.1696\n",
            "Epoch 11/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.1686 - mse: 1.1686\n",
            "Epoch 12/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.1677 - mse: 1.1677\n",
            "Epoch 13/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.1669 - mse: 1.1669\n",
            "Epoch 14/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1660 - mse: 1.1660\n",
            "Epoch 15/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1651 - mse: 1.1651\n",
            "Epoch 16/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.1643 - mse: 1.1643\n",
            "Epoch 17/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.1634 - mse: 1.1634\n",
            "Epoch 18/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1626 - mse: 1.1626\n",
            "Epoch 19/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1617 - mse: 1.1617\n",
            "Epoch 20/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.1609 - mse: 1.1609\n",
            "Epoch 21/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1.1600 - mse: 1.1600\n",
            "Epoch 22/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1592 - mse: 1.1592\n",
            "Epoch 23/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1584 - mse: 1.1584\n",
            "Epoch 24/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.1576 - mse: 1.1576\n",
            "Epoch 25/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1568 - mse: 1.1568\n",
            "Epoch 26/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1560 - mse: 1.1560\n",
            "Epoch 27/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1552 - mse: 1.1552\n",
            "Epoch 28/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1544 - mse: 1.1544\n",
            "Epoch 29/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.1537 - mse: 1.1537\n",
            "Epoch 30/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1529 - mse: 1.1529\n",
            "Epoch 31/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1521 - mse: 1.1521\n",
            "Epoch 32/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1514 - mse: 1.1514\n",
            "Epoch 33/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1506 - mse: 1.1506\n",
            "Epoch 34/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1499 - mse: 1.1499\n",
            "Epoch 35/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.1491 - mse: 1.1491\n",
            "Epoch 36/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.1484 - mse: 1.1484\n",
            "Epoch 37/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.1477 - mse: 1.1477\n",
            "Epoch 38/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.1470 - mse: 1.1470\n",
            "Epoch 39/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1462 - mse: 1.1462\n",
            "Epoch 40/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1455 - mse: 1.1455\n",
            "Epoch 41/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.1448 - mse: 1.1448\n",
            "Epoch 42/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.1441 - mse: 1.1441\n",
            "Epoch 43/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.1435 - mse: 1.1435\n",
            "Epoch 44/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1428 - mse: 1.1428\n",
            "Epoch 45/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1421 - mse: 1.1421\n",
            "Epoch 46/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1414 - mse: 1.1414\n",
            "Epoch 47/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1407 - mse: 1.1407\n",
            "Epoch 48/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.1401 - mse: 1.1401\n",
            "Epoch 49/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1394 - mse: 1.1394\n",
            "Epoch 50/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.1388 - mse: 1.1388\n",
            "Epoch 51/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1381 - mse: 1.1381\n",
            "Epoch 52/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.1375 - mse: 1.1375\n",
            "Epoch 53/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.1368 - mse: 1.1368\n",
            "Epoch 54/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.1362 - mse: 1.1362\n",
            "Epoch 55/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.1356 - mse: 1.1356\n",
            "Epoch 56/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1350 - mse: 1.1350\n",
            "Epoch 57/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1344 - mse: 1.1344\n",
            "Epoch 58/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.1337 - mse: 1.1337\n",
            "Epoch 59/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.1331 - mse: 1.1331\n",
            "Epoch 60/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1.1325 - mse: 1.1325\n",
            "Epoch 61/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.1319 - mse: 1.1319\n",
            "Epoch 62/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1.1314 - mse: 1.1314\n",
            "Epoch 63/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.1308 - mse: 1.1308\n",
            "Epoch 64/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1302 - mse: 1.1302\n",
            "Epoch 65/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.1296 - mse: 1.1296\n",
            "Epoch 66/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.1290 - mse: 1.1290\n",
            "Epoch 67/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1285 - mse: 1.1285\n",
            "Epoch 68/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1279 - mse: 1.1279\n",
            "Epoch 69/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.1273 - mse: 1.1273\n",
            "Epoch 70/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.1268 - mse: 1.1268\n",
            "Epoch 71/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1.1262 - mse: 1.1262\n",
            "Epoch 72/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.1257 - mse: 1.1257\n",
            "Epoch 73/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.1252 - mse: 1.1252\n",
            "Epoch 74/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.1246 - mse: 1.1246\n",
            "Epoch 75/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1241 - mse: 1.1241\n",
            "Epoch 76/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.1236 - mse: 1.1236\n",
            "Epoch 77/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.1230 - mse: 1.1230\n",
            "Epoch 78/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.1225 - mse: 1.1225\n",
            "Epoch 79/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1.1220 - mse: 1.1220\n",
            "Epoch 80/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.1215 - mse: 1.1215\n",
            "Epoch 81/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.1210 - mse: 1.1210\n",
            "Epoch 82/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1205 - mse: 1.1205\n",
            "Epoch 83/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.1200 - mse: 1.1200\n",
            "Epoch 84/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.1195 - mse: 1.1195\n",
            "Epoch 85/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.1190 - mse: 1.1190\n",
            "Epoch 86/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.1185 - mse: 1.1185\n",
            "Epoch 87/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1181 - mse: 1.1181\n",
            "Epoch 88/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.1176 - mse: 1.1176\n",
            "Epoch 89/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1171 - mse: 1.1171\n",
            "Epoch 90/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.1166 - mse: 1.1166\n",
            "Epoch 91/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.1162 - mse: 1.1162\n",
            "Epoch 92/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.1157 - mse: 1.1157\n",
            "Epoch 93/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1153 - mse: 1.1153\n",
            "Epoch 94/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1148 - mse: 1.1148\n",
            "Epoch 95/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.1144 - mse: 1.1144\n",
            "Epoch 96/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.1139 - mse: 1.1139\n",
            "Epoch 97/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.1135 - mse: 1.1135\n",
            "Epoch 98/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.1130 - mse: 1.1130\n",
            "Epoch 99/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1.1126 - mse: 1.1126\n",
            "Epoch 100/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1122 - mse: 1.1122\n",
            "Epoch 101/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.1117 - mse: 1.1117\n",
            "Epoch 102/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1113 - mse: 1.1113\n",
            "Epoch 103/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1109 - mse: 1.1109\n",
            "Epoch 104/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.1105 - mse: 1.1105\n",
            "Epoch 105/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.1100 - mse: 1.1100\n",
            "Epoch 106/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1096 - mse: 1.1096\n",
            "Epoch 107/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1092 - mse: 1.1092\n",
            "Epoch 108/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.1088 - mse: 1.1088\n",
            "Epoch 109/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.1084 - mse: 1.1084\n",
            "Epoch 110/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1080 - mse: 1.1080\n",
            "Epoch 111/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1076 - mse: 1.1076\n",
            "Epoch 112/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.1072 - mse: 1.1072\n",
            "Epoch 113/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1068 - mse: 1.1068\n",
            "Epoch 114/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.1065 - mse: 1.1065\n",
            "Epoch 115/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1061 - mse: 1.1061\n",
            "Epoch 116/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1057 - mse: 1.1057\n",
            "Epoch 117/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.1053 - mse: 1.1053\n",
            "Epoch 118/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.1050 - mse: 1.1050\n",
            "Epoch 119/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1046 - mse: 1.1046\n",
            "Epoch 120/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1042 - mse: 1.1042\n",
            "Epoch 121/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.1039 - mse: 1.1039\n",
            "Epoch 122/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1035 - mse: 1.1035\n",
            "Epoch 123/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.1031 - mse: 1.1031\n",
            "Epoch 124/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1028 - mse: 1.1028\n",
            "Epoch 125/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1024 - mse: 1.1024\n",
            "Epoch 126/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.1021 - mse: 1.1021\n",
            "Epoch 127/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1017 - mse: 1.1017\n",
            "Epoch 128/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.1014 - mse: 1.1014\n",
            "Epoch 129/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.1010 - mse: 1.1010\n",
            "Epoch 130/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1007 - mse: 1.1007\n",
            "Epoch 131/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1004 - mse: 1.1004\n",
            "Epoch 132/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.1000 - mse: 1.1000\n",
            "Epoch 133/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.0997 - mse: 1.0997\n",
            "Epoch 134/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0994 - mse: 1.0994\n",
            "Epoch 135/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0991 - mse: 1.0991\n",
            "Epoch 136/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.0987 - mse: 1.0987\n",
            "Epoch 137/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.0984 - mse: 1.0984\n",
            "Epoch 138/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0981 - mse: 1.0981\n",
            "Epoch 139/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0978 - mse: 1.0978\n",
            "Epoch 140/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0975 - mse: 1.0975\n",
            "Epoch 141/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0972 - mse: 1.0972\n",
            "Epoch 142/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0969 - mse: 1.0969\n",
            "Epoch 143/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.0966 - mse: 1.0966\n",
            "Epoch 144/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.0963 - mse: 1.0963\n",
            "Epoch 145/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0960 - mse: 1.0960\n",
            "Epoch 146/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.0957 - mse: 1.0957\n",
            "Epoch 147/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.0954 - mse: 1.0954\n",
            "Epoch 148/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.0951 - mse: 1.0951\n",
            "Epoch 149/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0948 - mse: 1.0948\n",
            "Epoch 150/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0945 - mse: 1.0945\n",
            "Epoch 151/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0942 - mse: 1.0942\n",
            "Epoch 152/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0939 - mse: 1.0939\n",
            "Epoch 153/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.0937 - mse: 1.0937\n",
            "Epoch 154/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.0934 - mse: 1.0934\n",
            "Epoch 155/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.0931 - mse: 1.0931\n",
            "Epoch 156/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.0928 - mse: 1.0928\n",
            "Epoch 157/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.0926 - mse: 1.0926\n",
            "Epoch 158/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.0923 - mse: 1.0923\n",
            "Epoch 159/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0920 - mse: 1.0920\n",
            "Epoch 160/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0918 - mse: 1.0918\n",
            "Epoch 161/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0915 - mse: 1.0915\n",
            "Epoch 162/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.0913 - mse: 1.0913\n",
            "Epoch 163/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.0910 - mse: 1.0910\n",
            "Epoch 164/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0907 - mse: 1.0907\n",
            "Epoch 165/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0905 - mse: 1.0905\n",
            "Epoch 166/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.0902 - mse: 1.0902\n",
            "Epoch 167/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0900 - mse: 1.0900\n",
            "Epoch 168/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.0897 - mse: 1.0897\n",
            "Epoch 169/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0895 - mse: 1.0895\n",
            "Epoch 170/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.0893 - mse: 1.0893\n",
            "Epoch 171/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0890 - mse: 1.0890\n",
            "Epoch 172/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.0888 - mse: 1.0888\n",
            "Epoch 173/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1.0885 - mse: 1.0885\n",
            "Epoch 174/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.0883 - mse: 1.0883\n",
            "Epoch 175/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1.0881 - mse: 1.0881\n",
            "Epoch 176/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1.0878 - mse: 1.0878\n",
            "Epoch 177/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.0876 - mse: 1.0876\n",
            "Epoch 178/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.0874 - mse: 1.0874\n",
            "Epoch 179/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0872 - mse: 1.0872\n",
            "Epoch 180/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.0869 - mse: 1.0869\n",
            "Epoch 181/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0867 - mse: 1.0867\n",
            "Epoch 182/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.0865 - mse: 1.0865\n",
            "Epoch 183/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.0863 - mse: 1.0863\n",
            "Epoch 184/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.0861 - mse: 1.0861\n",
            "Epoch 185/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.0858 - mse: 1.0858\n",
            "Epoch 186/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.0856 - mse: 1.0856\n",
            "Epoch 187/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.0854 - mse: 1.0854\n",
            "Epoch 188/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0852 - mse: 1.0852\n",
            "Epoch 189/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0850 - mse: 1.0850\n",
            "Epoch 190/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0848 - mse: 1.0848\n",
            "Epoch 191/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0846 - mse: 1.0846\n",
            "Epoch 192/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0844 - mse: 1.0844\n",
            "Epoch 193/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.0842 - mse: 1.0842\n",
            "Epoch 194/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0840 - mse: 1.0840\n",
            "Epoch 195/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0838 - mse: 1.0838\n",
            "Epoch 196/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.0836 - mse: 1.0836\n",
            "Epoch 197/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.0834 - mse: 1.0834\n",
            "Epoch 198/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0832 - mse: 1.0832\n",
            "Epoch 199/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0830 - mse: 1.0830\n",
            "Epoch 200/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0828 - mse: 1.0828\n",
            "Epoch 201/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0826 - mse: 1.0826\n",
            "Epoch 202/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0825 - mse: 1.0825\n",
            "Epoch 203/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0823 - mse: 1.0823\n",
            "Epoch 204/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0821 - mse: 1.0821\n",
            "Epoch 205/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.0819 - mse: 1.0819\n",
            "Epoch 206/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.0817 - mse: 1.0817\n",
            "Epoch 207/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0815 - mse: 1.0815\n",
            "Epoch 208/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.0814 - mse: 1.0814\n",
            "Epoch 209/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0812 - mse: 1.0812\n",
            "Epoch 210/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0810 - mse: 1.0810\n",
            "Epoch 211/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0808 - mse: 1.0808\n",
            "Epoch 212/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0807 - mse: 1.0807\n",
            "Epoch 213/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0805 - mse: 1.0805\n",
            "Epoch 214/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0803 - mse: 1.0803\n",
            "Epoch 215/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0802 - mse: 1.0802\n",
            "Epoch 216/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.0800 - mse: 1.0800\n",
            "Epoch 217/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0798 - mse: 1.0798\n",
            "Epoch 218/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0797 - mse: 1.0797\n",
            "Epoch 219/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 1.0795 - mse: 1.0795\n",
            "Epoch 220/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0793 - mse: 1.0793\n",
            "Epoch 221/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0792 - mse: 1.0792\n",
            "Epoch 222/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.0790 - mse: 1.0790\n",
            "Epoch 223/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.0789 - mse: 1.0789\n",
            "Epoch 224/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.0787 - mse: 1.0787\n",
            "Epoch 225/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.0786 - mse: 1.0786\n",
            "Epoch 226/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.0784 - mse: 1.0784\n",
            "Epoch 227/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.0783 - mse: 1.0783\n",
            "Epoch 228/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1.0781 - mse: 1.0781\n",
            "Epoch 229/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.0780 - mse: 1.0780\n",
            "Epoch 230/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1.0778 - mse: 1.0778\n",
            "Epoch 231/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0777 - mse: 1.0777\n",
            "Epoch 232/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.0775 - mse: 1.0775\n",
            "Epoch 233/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0774 - mse: 1.0774\n",
            "Epoch 234/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0772 - mse: 1.0772\n",
            "Epoch 235/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1.0771 - mse: 1.0771\n",
            "Epoch 236/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0769 - mse: 1.0769\n",
            "Epoch 237/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.0768 - mse: 1.0768\n",
            "Epoch 238/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0767 - mse: 1.0767\n",
            "Epoch 239/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0765 - mse: 1.0765\n",
            "Epoch 240/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1.0764 - mse: 1.0764\n",
            "Epoch 241/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0763 - mse: 1.0763\n",
            "Epoch 242/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.0761 - mse: 1.0761\n",
            "Epoch 243/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0760 - mse: 1.0760\n",
            "Epoch 244/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.0759 - mse: 1.0759\n",
            "Epoch 245/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0757 - mse: 1.0757\n",
            "Epoch 246/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.0756 - mse: 1.0756\n",
            "Epoch 247/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.0755 - mse: 1.0755\n",
            "Epoch 248/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.0753 - mse: 1.0753\n",
            "Epoch 249/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0752 - mse: 1.0752\n",
            "Epoch 250/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.0751 - mse: 1.0751\n",
            "Epoch 251/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.0750 - mse: 1.0750\n",
            "Epoch 252/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0748 - mse: 1.0748\n",
            "Epoch 253/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0747 - mse: 1.0747\n",
            "Epoch 254/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0746 - mse: 1.0746\n",
            "Epoch 255/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0745 - mse: 1.0745\n",
            "Epoch 256/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0743 - mse: 1.0743\n",
            "Epoch 257/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.0742 - mse: 1.0742\n",
            "Epoch 258/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1.0741 - mse: 1.0741\n",
            "Epoch 259/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0740 - mse: 1.0740\n",
            "Epoch 260/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.0739 - mse: 1.0739\n",
            "Epoch 261/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.0738 - mse: 1.0738\n",
            "Epoch 262/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.0737 - mse: 1.0737\n",
            "Epoch 263/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.0735 - mse: 1.0735\n",
            "Epoch 264/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.0734 - mse: 1.0734\n",
            "Epoch 265/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0733 - mse: 1.0733\n",
            "Epoch 266/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0732 - mse: 1.0732\n",
            "Epoch 267/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0731 - mse: 1.0731\n",
            "Epoch 268/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.0730 - mse: 1.0730\n",
            "Epoch 269/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.0729 - mse: 1.0729\n",
            "Epoch 270/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1.0728 - mse: 1.0728\n",
            "Epoch 271/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0727 - mse: 1.0727\n",
            "Epoch 272/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.0726 - mse: 1.0726\n",
            "Epoch 273/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.0725 - mse: 1.0725\n",
            "Epoch 274/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0724 - mse: 1.0724\n",
            "Epoch 275/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.0723 - mse: 1.0723\n",
            "Epoch 276/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0722 - mse: 1.0722\n",
            "Epoch 277/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1.0720 - mse: 1.0720\n",
            "Epoch 278/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0720 - mse: 1.0720\n",
            "Epoch 279/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0719 - mse: 1.0719\n",
            "Epoch 280/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.0718 - mse: 1.0718\n",
            "Epoch 281/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.0717 - mse: 1.0717\n",
            "Epoch 282/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0716 - mse: 1.0716\n",
            "Epoch 283/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.0715 - mse: 1.0715\n",
            "Epoch 284/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0714 - mse: 1.0714\n",
            "Epoch 285/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0713 - mse: 1.0713\n",
            "Epoch 286/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0712 - mse: 1.0712\n",
            "Epoch 287/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.0711 - mse: 1.0711\n",
            "Epoch 288/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0710 - mse: 1.0710\n",
            "Epoch 289/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0709 - mse: 1.0709\n",
            "Epoch 290/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1.0708 - mse: 1.0708\n",
            "Epoch 291/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0707 - mse: 1.0707\n",
            "Epoch 292/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0706 - mse: 1.0706\n",
            "Epoch 293/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0706 - mse: 1.0706\n",
            "Epoch 294/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.0705 - mse: 1.0705\n",
            "Epoch 295/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0704 - mse: 1.0704\n",
            "Epoch 296/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0703 - mse: 1.0703\n",
            "Epoch 297/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0702 - mse: 1.0702\n",
            "Epoch 298/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.0701 - mse: 1.0701\n",
            "Epoch 299/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0700 - mse: 1.0700\n",
            "Epoch 300/300\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0700 - mse: 1.0700\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x2309fc27580>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "# 입력 x의 차원은 1, 출력 y의 차원도 1. 선형 회귀이므로 activation은 'linear'\n",
        "model.add(Dense(1, input_dim=1, activation='linear'))\n",
        "\n",
        "# sgd는 경사 하강법을 의미. 학습률(learning rate, lr)은 0.01.\n",
        "# sgd = optimizers.SGD(lr=0.01)\n",
        "sgd = optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "# 손실 함수(Loss function)은 평균제곱오차 mse를 사용합니다.\n",
        "model.compile(optimizer=sgd, loss='mse', metrics=['mse'])\n",
        "\n",
        "# 주어진 x와 y데이터에 대해서 오차를 최소화하는 작업을 300번 시도합니다.\n",
        "model.fit(x, y, epochs=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "학습이 끝났습니다. 최종적으로 선택된 오차를 최소화하는 직선을 그래프로 그려보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "GowvMQMUNht7",
        "outputId": "6c9e109b-a52f-4697-b39e-a289791ae4f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x230a01f9e70>,\n",
              " <matplotlib.lines.Line2D at 0x230a01f9fc0>]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(x, model.predict(x), 'b', x, y, 'k.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "위의 그래프에서 각 점은 우리가 실제 주었던 실제값에 해당되며, 직선은 실제값으로부터 오차를 최소화하는 \n",
        "와 \n",
        "의 값을 가지는 직선입니다. 이 직선을 통해 9시간 30분을 공부하였을 때의 시험 성적을 예측하게 해봅시다. model.predict()은 학습이 완료된 모델이 입력된 데이터에 대해서 어떤 값을 예측하는지를 보여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "[[91.53745]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(model.predict(np.array([8.5])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8시간 30분을 공부하면 약 91.5점을 얻는다고 예측하고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "마지막 편집일시 : 2022년 11월 14일 2:48 오후"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "자동 미분과 선형 회귀.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py310_yolo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
