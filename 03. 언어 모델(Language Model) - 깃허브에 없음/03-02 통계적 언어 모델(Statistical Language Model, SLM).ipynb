{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1LeJfJIyCO1"
   },
   "source": [
    "## **03-02 통계적 언어 모델(Statistical Language Model, SLM)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kUnEfPWyGy1"
   },
   "source": [
    "언어 모델의 전통적인 접근 방법인 통계적 언어 모델을 소개합니다. 통계적 언어 모델이 통계적인 접근 방법으로 어떻게 언어를 모델링 하는지 배워보겠습니다. 통계적 언어 모델(Statistical Language Model)은 줄여서 SLM이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lzs66L4uyItv"
   },
   "source": [
    "---\n",
    "### **1. 조건부 확률**\n",
    "\n",
    "조건부 확률은 두 확률 $P(A),\\ P(B)$에 대해서 아래와 같은 관계를 갖습니다.  \n",
    "  \n",
    "$$p(B|A) = P(A,B)/P(A)$$  \n",
    "$$P(A,B) = P(A)P(B|A)$$\n",
    "\n",
    "더 많은 확률에 대해서 일반화해봅시다. 4개의 확률이 조건부 확률의 관계를 가질 때, 아래와 같이 표현할 수 있습니다.\n",
    "  \n",
    "\n",
    "$$P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)$$\n",
    "  \n",
    "\n",
    "이를 **조건부 확률의 연쇄 법칙(chain rule)** 이라고 합니다. 이제는 4개가 아닌 $n$개에 대해서 일반화를 해봅시다.  \n",
    "  \n",
    "$$P(x_1, x_2, x_3 ... x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1 ... x_{n-1})$$    \n",
    "  \n",
    "    \n",
    "조건부 확률에 대한 정의를 통해 문장의 확률을 구해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycXIZ9fZy5pj"
   },
   "source": [
    "---\n",
    "### **2. 문장에 대한 확률**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEP36_PMy9x6"
   },
   "source": [
    "문장 'An adorable little boy is spreading smiles'의 확률 $P(\\text{An adorable little boy is spreading smiles})$를 식으로 표현해봅시다.\n",
    "\n",
    "각 단어는 **문맥** 이라는 관계로 인해 **이전 단어의 영향을 받아 나온 단어** 입니다. 그리고 모든 단어로부터 하나의 **문장** 이 완성됩니다. 그렇기 때문에 **문장의 확률** 을 구하고자 조건부 확률을 사용하겠습니다. 앞서 언급한 조건부 확률의 일반화 식을 문장의 확률 관점에서 다시 적어보면 문장의 확률은 각 단어들이 이전 단어가 주어졌을 때 다음 단어로 등장할 확률의 곱으로 구성됩니다.\n",
    "  \n",
    "\n",
    "$$P(w_1, w_2, w_3, w_4, w_5, ... w_n) = \\prod_{n=1}^{n}P(w_{n} | w_{1}, ... , w_{n-1})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "위의 문장에 해당 식을 적용해보면 다음과 같습니다.  \n",
    "  \n",
    "$P(\\text{An adorable little boy is spreading smiles}) =$  \n",
    "  \n",
    "$P(\\text{An})  ×  P(\\text{adorable|An})  ×  P(\\text{little|An adorable})  ×  P(\\text{boy|An adorable little})\n",
    "         ×  P(\\text{is|An adorable little boy})$\n",
    "\n",
    "$×  P(\\text{spreading|An adorable little boy is})  ×  P(\\text{smiles|An adorable little boy is spreading})$\n",
    " \n",
    "\n",
    "**문장의 확률** 을 구하기 위해서 **각 단어에 대한 예측 확률들을 곱** 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1aldnZwzypb"
   },
   "source": [
    "---\n",
    "### **3. 카운트 기반의 접근**\n",
    "\n",
    "문장의 확률을 구하기 위해서 다음 단어에 대한 예측 확률을 모두 곱한다는 것은 알았습니다. 그렇다면 SLM은 이전 단어로부터 다음 단어에 대한 확률은 어떻게 구할까요? 정답은 카운트에 기반하여 확률을 계산합니다.\n",
    "\n",
    "**An adorable little boy가 나왔을 때, is가 나올 확률** 인 $P(\\text{is|An adorable little boy})$를 구해봅시다.  \n",
    "  \n",
    "$$P\\text{(is|An adorable little boy}) = \\frac{\\text{count(An adorable little boy is})}{\\text{count(An adorable little boy })}$$\n",
    "  \n",
    "그 확률은 위와 같습니다. 예를 들어 기계가 학습한 코퍼스 데이터에서 An adorable little boy가 100번 등장했는데 그 다음에 is가 등장한 경우는 30번이라고 합시다. 이 경우 \n",
    "는 30%입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl-wDH3Q0DW5"
   },
   "source": [
    "---\n",
    "### **4. 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DveWhdhT0Kd6"
   },
   "source": [
    "언어 모델은 **실생활에서 사용되는 언어의 확률 분포를 근사 모델링** 합니다. 실제로 정확하게 알아볼 방법은 없겠지만 현실에서도 An adorable little boy가 나왔을 때 is가 나올 확률이라는 것이 존재합니다. 이를 실제 자연어의 확률 분포, 현실에서의 확률 분포라고 명칭합시다. **기계에게 많은 코퍼스를 훈련** 시켜서 언어 모델을 통해 현실에서의 확률 분포를 근사하는 것이 언어 모델의 목표입니다. 그런데 카운트 기반으로 접근하려고 한다면 갖고있는 코퍼스(corpus). 즉, 다시 말해 기계가 훈련하는 데이터는 정말 **방대한 양이 필요** 합니다.  \n",
    "  \n",
    "$$P\\text{(is|An adorable little boy}) = \\frac{\\text{count(An adorable little boy is})}{\\text{count(An adorable little boy })}$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 위와 같이 $P\\text{(is|An adorable little boy})$를 구하는 경우에서 **기계가 훈련한 코퍼스에 An adorable little boy is라는 단어 시퀀스가 없었다면 이 단어 시퀀스에 대한 확률은 0** 이 됩니다. 또는 An adorable little boy라는 단어 시퀀스가 없었다면 **분모가 0** 이 되어 확률은 정의되지 않습니다. 그렇다면 코퍼스에 단어 시퀀스가 없다고 해서 이 확률을 0 또는 정의되지 않는 확률이라고 하는 것이 정확한 모델링 방법일까요? 아닙니다. 현실에선 An adorable little boy is 라는 단어 시퀀스가 존재하고 또 문법에도 적합하므로 정답일 가능성 또한 높습니다. 이와 같이 **충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제** 를 **희소 문제(sparsity problem)** 라고 합니다.  \n",
    "  \n",
    "위 문제를 **완화하는 방법** 으로 바로 이어서 배우게 되는 **n-gram** 언어 모델이나 이 책에서 다루지는 않지만 **스무딩** 이나 **백오프** 와 같은 여러가지 **일반화(generalization) 기법** 이 존재합니다. 하지만 희소 문제에 대한 근본적인 해결책은 되지 못하였습니다. 결국 이러한 한계로 인해 언어 모델의 트렌드는 통계적 언어 모델에서 인공 신경망 언어 모델로 넘어가게 됩니다.\n",
    "  \n",
    "마지막 편집일시 : 2022년 11월 14일 2:44 오후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lImQ8Wwr3s0A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM/wNxe/GW9fK4g8gz/8BEm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
