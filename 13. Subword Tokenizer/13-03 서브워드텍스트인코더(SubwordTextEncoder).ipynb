{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jbm7FksfnaR-"
   },
   "source": [
    "2021년 12월 24일에 마지막으로 실행 확인하였습니다.  \n",
    "위키독스 '딥 러닝을 이용한 자연어 처리 입문'의 서브워드 텍스트 인코더 튜토리얼입니다.  \n",
    "\n",
    "링크 : https://wikidocs.net/86792"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MrHNLyt6FPP"
   },
   "source": [
    "## **13-03 서브워드텍스트인코더(SubwordTextEncoder)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZzLSu2X6FPQ"
   },
   "source": [
    "SubwordTextEncoder는 텐서플로우를 통해 사용할 수 있는 서브워드 토크나이저입니다. BPE와 유사한 알고리즘인 Wordpiece Model을 채택하였으며, 패키지를 통해 쉽게 단어들을 서브워드들로 분리할 수 있습니다. SubwordTextEncoder를 통해서 IMDB 영화 리뷰 데이터와 네이버 영화 리뷰 데이터에 대해서 토큰화 작업을 수행해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB5Z9MiT6FPQ"
   },
   "source": [
    "### **1. IMDB 리뷰 토큰화하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Using cached tensorflow_datasets-4.9.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from tensorflow-datasets) (2.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from tensorflow-datasets) (8.1.7)\n",
      "Collecting dm-tree (from tensorflow-datasets)\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting immutabledict (from tensorflow-datasets)\n",
      "  Using cached immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from tensorflow-datasets) (1.26.4)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from tensorflow-datasets) (4.25.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from tensorflow-datasets) (5.9.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from tensorflow-datasets) (17.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from tensorflow-datasets) (2.32.2)\n",
      "Collecting simple-parsing (from tensorflow-datasets)\n",
      "  Using cached simple_parsing-0.1.5-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Using cached tensorflow_metadata-1.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from tensorflow-datasets) (2.4.0)\n",
      "Collecting toml (from tensorflow-datasets)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from tensorflow-datasets) (4.66.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from tensorflow-datasets) (1.16.0)\n",
      "Collecting etils>=1.6.0 (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets)\n",
      "  Downloading etils-1.7.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting fsspec (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (6.4.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (4.11.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (3.19.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from click->tensorflow-datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\jeong\\anaconda3-1\\envs\\py310\\lib\\site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Collecting docstring-parser~=0.15 (from simple-parsing->tensorflow-datasets)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting protobuf>=3.20 (from tensorflow-datasets)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl.metadata (698 bytes)\n",
      "Using cached tensorflow_datasets-4.9.6-py3-none-any.whl (5.1 MB)\n",
      "Downloading etils-1.7.0-py3-none-any.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/152.4 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 143.4/152.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 152.4/152.4 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading dm_tree-0.1.8-cp310-cp310-win_amd64.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 101.3/101.3 kB 5.7 MB/s eta 0:00:00\n",
      "Using cached immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached simple_parsing-0.1.5-py3-none-any.whl (113 kB)\n",
      "Using cached tensorflow_metadata-1.15.0-py3-none-any.whl (28 kB)\n",
      "Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "   ---------------------------------------- 0.0/904.0 kB ? eta -:--:--\n",
      "   -------------------- ------------------ 471.0/904.0 kB 14.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 904.0/904.0 kB 11.5 MB/s eta 0:00:00\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21544 sha256=750ecfe5ce4b16364d0dba0071cb3c83c64af11c2086dec647ca910ecada7583\n",
      "  Stored in directory: c:\\users\\jeong\\appdata\\local\\pip\\cache\\wheels\\54\\4e\\28\\3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, toml, protobuf, promise, immutabledict, fsspec, etils, docstring-parser, tensorflow-metadata, simple-parsing, tensorflow-datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "Successfully installed dm-tree-0.1.8 docstring-parser-0.16 etils-1.7.0 fsspec-2024.6.1 immutabledict-4.2.0 promise-2.3 protobuf-3.20.3 simple-parsing-0.1.5 tensorflow-datasets-4.9.6 tensorflow-metadata-1.15.0 toml-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\jeong\\anaconda3-1\\envs\\py310\\Lib\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9723,
     "status": "ok",
     "timestamp": 1674498966994,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "ZoI4avtrX9pl"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import urllib.request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1674498966998,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "daYwOzN0n93y",
    "outputId": "487e9b19-83a1-42b2-c605-61c1be24ec6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.17.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__  # tf 2.17.0  2024.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r51nTFdI6FPS"
   },
   "source": [
    "다운로드한 데이터를 데이터프레임에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3409,
     "status": "ok",
     "timestamp": 1674498970386,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "RzMkNB9vYgzX",
    "outputId": "6cf80cf9-aece-4720-e49d-e96de0a612f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('IMDb_Reviews.csv', <http.client.HTTPMessage at 0x1f16d860550>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 1408,
     "status": "ok",
     "timestamp": 1674498971771,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "8t2ygF8KY1Ff",
    "outputId": "fed61487-0f9c-478a-cd19-e058556d0091"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>the people who came up with this are SICK AND ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The script is so so laughable... this in turn,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>\"So there's this bride, you see, and she gets ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Your mind will not be satisfied by this nobud...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>The chaser's war on everything is a weekly sho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      My family and I normally do not watch local mo...          1\n",
       "1      Believe it or not, this was at one time the wo...          0\n",
       "2      After some internet surfing, I found the \"Home...          0\n",
       "3      One of the most unheralded great works of anim...          1\n",
       "4      It was the Sixties, and anyone with long hair ...          0\n",
       "...                                                  ...        ...\n",
       "49995  the people who came up with this are SICK AND ...          0\n",
       "49996  The script is so so laughable... this in turn,...          0\n",
       "49997  \"So there's this bride, you see, and she gets ...          0\n",
       "49998  Your mind will not be satisfied by this nobud...          0\n",
       "49999  The chaser's war on everything is a weekly sho...          1\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('IMDb_Reviews.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggt65v3Y6FPT"
   },
   "source": [
    "데이터프레임에서 'review'에 해당하는 열이 토큰화를 수행해야 할 데이터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1674499032335,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "d4a8-uJ6Y52P",
    "outputId": "3800d538-9b80-49ef-925d-aefc9505dc6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        My family and I normally do not watch local mo...\n",
       "1        Believe it or not, this was at one time the wo...\n",
       "2        After some internet surfing, I found the \"Home...\n",
       "3        One of the most unheralded great works of anim...\n",
       "4        It was the Sixties, and anyone with long hair ...\n",
       "                               ...                        \n",
       "49995    the people who came up with this are SICK AND ...\n",
       "49996    The script is so so laughable... this in turn,...\n",
       "49997    \"So there's this bride, you see, and she gets ...\n",
       "49998    Your mind will not be satisfied by this nobud...\n",
       "49999    The chaser's war on everything is a weekly sho...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTAyXorZ6FPT"
   },
   "source": [
    "tfds.features.text.SubwordTextEncoder.build_from_corpus의 인자로 토큰화 할 데이터를 넣어줍니다. 이 작업을 통해서 서브워드들로 이루어진 단어 집합(Vocabulary)를 생성하고, 각 서브워드에 고유한 정수를 부여합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 338586,
     "status": "ok",
     "timestamp": 1674499373730,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "hQpdGiUiY_bf"
   },
   "outputs": [],
   "source": [
    "### pc cpu : 4분\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "                            train_df['review'], target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juqN2MZE6FPU"
   },
   "source": [
    ".subwords를 통해서 토큰화 된 서브워드들을 확인할 수 있습니다. 100개만 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1674499373731,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "uZj744JQabta",
    "outputId": "0d6928e2-cff7-4079-e033-b8a26e9d5e4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 't_', 'as_', 'with_', 'for_', '.<', 'on_', 'but_', 'movie_', 'are_', ' (', 'have_', 'his_', 'film_', 'not_', 'be_', 'you_', 'ing_', ' \"', 'ed_', 'it', 'd_', 'an_', 'at_', 'by_', 'he_', 'one_', 'who_', 'from_', 'y_', 'or_', 'e_', 'like_', 'all_', '\" ', 'they_', 'so_', 'just_', 'has_', ') ', 'about_', 'her_', 'out_', 'This_', 'some_', 'movie', 'ly_', 'film', 'very_', 'more_', 'It_', 'what_', 'would_', 'when_', 'if_', 'good_', 'up_', 'which_', 'their_', 'only_', 'even_', 'my_', 'really_', 'had_', 'can_', 'no_', 'were_', 'see_', '? ', 'she_', 'than_', '! ', 'there_', 'been_', 'get_', 'into_', 'will_', ' - ', 'much_', 'n_', 'because_', 'ing']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.subwords[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjL1pftH6FPU"
   },
   "source": [
    "임의로 선택한 20번 인덱스의 샘플을 출력해보고, 정수 인코딩을 수행한 결과와 비교해보겠습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBHZXjPK1lW7",
    "outputId": "a1e75076-1e91-4922-8233-a3fe9b31ccc0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein's monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)\n"
     ]
    }
   ],
   "source": [
    "print(train_df['review'][20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFt7kwNg6FPV"
   },
   "source": [
    "encode()를 통해서 입력한 데이터에 대해서 정수 인코딩을 수행한 결과를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dkHSWWjZNA_",
    "outputId": "828f17d4-bc49-4be6-e7de-2f6f53b07976",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sample question: [1590, 4162, 132, 7107, 1892, 2983, 578, 76, 12, 4632, 3422, 7, 160, 175, 372, 2, 5, 39, 8051, 8, 84, 2652, 497, 39, 8051, 8, 1374, 5, 3461, 2012, 48, 5, 2263, 21, 4, 2992, 127, 4729, 711, 3, 1391, 8044, 3557, 1277, 8102, 2154, 5681, 9, 42, 15, 372, 2, 3773, 4, 3502, 2308, 467, 4890, 1503, 11, 3347, 1419, 8127, 29, 5539, 98, 6099, 58, 94, 4, 1388, 4230, 8057, 213, 3, 1966, 2, 1, 6700, 8044, 9, 7069, 716, 8057, 6600, 2, 4102, 36, 78, 6, 4, 1865, 40, 5, 3502, 1043, 1645, 8044, 1000, 1813, 23, 1, 105, 1128, 3, 156, 15, 85, 33, 23, 8102, 2154, 5681, 5, 6099, 8051, 8, 7271, 1055, 2, 534, 22, 1, 3046, 5214, 810, 634, 8120, 2, 14, 71, 34, 436, 3311, 5447, 783, 3, 6099, 2, 46, 71, 193, 25, 7, 428, 2274, 2260, 6487, 8051, 8, 2149, 23, 1138, 4117, 6023, 163, 11, 148, 735, 2, 164, 4, 5277, 921, 3395, 1262, 37, 639, 1349, 349, 5, 2460, 328, 15, 5349, 8127, 24, 10, 16, 10, 17, 8054, 8061, 8059, 8062, 29, 6, 6607, 8126, 8053]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sample question: {}'.format(tokenizer.encode(train_df['review'][20])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNpxJVxM6FPV"
   },
   "source": [
    "위의 결과를 decode()로 다시 문장으로~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PwFrkKRl6FPV",
    "outputId": "5119ed22-7d91-4261-cd78-220b46f7329b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pretty bad PRC cheapie which I rarely bother to watch over again, and it\\'s no wonder -- it\\'s slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange\\'s presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein\\'s monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1590, 4162, 132, 7107, 1892, 2983, 578, 76, 12, 4632, 3422, 7, 160, 175, 372, 2, 5, 39, 8051, 8, 84, 2652, 497, 39, 8051, 8, 1374, 5, 3461, 2012, 48, 5, 2263, 21, 4, 2992, 127, 4729, 711, 3, 1391, 8044, 3557, 1277, 8102, 2154, 5681, 9, 42, 15, 372, 2, 3773, 4, 3502, 2308, 467, 4890, 1503, 11, 3347, 1419, 8127, 29, 5539, 98, 6099, 58, 94, 4, 1388, 4230, 8057, 213, 3, 1966, 2, 1, 6700, 8044, 9, 7069, 716, 8057, 6600, 2, 4102, 36, 78, 6, 4, 1865, 40, 5, 3502, 1043, 1645, 8044, 1000, 1813, 23, 1, 105, 1128, 3, 156, 15, 85, 33, 23, 8102, 2154, 5681, 5, 6099, 8051, 8, 7271, 1055, 2, 534, 22, 1, 3046, 5214, 810, 634, 8120, 2, 14, 71, 34, 436, 3311, 5447, 783, 3, 6099, 2, 46, 71, 193, 25, 7, 428, 2274, 2260, 6487, 8051, 8, 2149, 23, 1138, 4117, 6023, 163, 11, 148, 735, 2, 164, 4, 5277, 921, 3395, 1262, 37, 639, 1349, 349, 5, 2460, 328, 15, 5349, 8127, 24, 10, 16, 10, 17, 8054, 8061, 8059, 8062, 29, 6, 6607, 8126, 8053])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86F4qeNt6FPW"
   },
   "source": [
    "임의로 선택한 짧은 문장에 대해서 정수 인코딩 결과를 확인하고, 이를 다시 역으로 디코딩해보겠습니다. 디코딩 할 때는 인코딩할 때 encode()를 사용한 것과 유사하게 decode()를 통해서 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXdSlmhoabJR",
    "outputId": "bb523f68-7923-4dd1-afdd-d4579bc4600e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 : [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 79, 681, 8058]\n"
     ]
    }
   ],
   "source": [
    "# train_df에 존재하는 문장 중 일부를 발췌\n",
    "sample_string = \"It's mind-blowing to me that this film was even made.\"\n",
    "\n",
    "# 인코딩한 결과를 tokenized_string에 저장\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ir411Lg6FPW",
    "outputId": "bb523f68-7923-4dd1-afdd-d4579bc4600e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 문장 : It's mind-blowing to me that this film was even made.\n"
     ]
    }
   ],
   "source": [
    "# 이를 다시 디코딩\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장 : {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3p4zQxL6FPX"
   },
   "source": [
    ".vocab_size를 통해 단어 집합의 크기를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76mGh70lal8o",
    "outputId": "42c80875-ae23-4f95-82e6-673675557fd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기(Vocab size) : 8268\n"
     ]
    }
   ],
   "source": [
    "print('단어 집합의 크기(Vocab size) :', tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBHL4sDA6FPX"
   },
   "source": [
    "현재 단어 집합의 크기는 8,268개입니다. 디코딩 결과를 병렬적으로 나열하여 각 단어와 맵핑된 정수를 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "x3buGB8u6FPX",
    "outputId": "ed23fb9e-5b93-4282-c9c3-5f62e01e5751"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 79, 681, 8058]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9W054Sf6Zjw3",
    "outputId": "986d19dd-b813-4738-f54c-25941ffb8422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ----> It\n",
      "8051 ----> '\n",
      "8 ----> s \n",
      "910 ----> mind\n",
      "8057 ----> -\n",
      "2169 ----> blow\n",
      "36 ----> ing \n",
      "7 ----> to \n",
      "103 ----> me \n",
      "13 ----> that \n",
      "14 ----> this \n",
      "32 ----> film \n",
      "18 ----> was \n",
      "79 ----> even \n",
      "681 ----> made\n",
      "8058 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqY6S77G6FPX"
   },
   "source": [
    "이번에는 기존 예제 문장 중 even이라는 단어에 임의로 xyz라는 3개의 글자를 추가해봤습니다. 현재 토크나이저가 even이라는 단어를 이미 하나의 서브워드로 인식하고 있는 상황에서 나머지 xyz를 어떻게 분리하는지를 확인하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "piGvKELplgNU",
    "outputId": "2fd3c52b-d5c2-4ebc-d023-7a42e580e412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 7974, 8132, 8133, 997, 681, 8058]\n"
     ]
    }
   ],
   "source": [
    "sample_string = \"It's mind-blowing to me that this film was evenxyz made.\"\n",
    "\n",
    "# encode\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1NqrwqV6FPY",
    "outputId": "2fd3c52b-d5c2-4ebc-d023-7a42e580e412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 문장: It's mind-blowing to me that this film was evenxyz made.\n"
     ]
    }
   ],
   "source": [
    "# encoding한 문장을 다시 decode\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "IgeGrnPn6FPY"
   },
   "outputs": [],
   "source": [
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JCAXr27vSuI",
    "outputId": "c31afd21-e8ba-4a39-f710-78efd53385a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ----> It\n",
      "8051 ----> '\n",
      "8 ----> s \n",
      "910 ----> mind\n",
      "8057 ----> -\n",
      "2169 ----> blow\n",
      "36 ----> ing \n",
      "7 ----> to \n",
      "103 ----> me \n",
      "13 ----> that \n",
      "14 ----> this \n",
      "32 ----> film \n",
      "18 ----> was \n",
      "7974 ----> even\n",
      "8132 ----> x\n",
      "8133 ----> y\n",
      "997 ----> z \n",
      "681 ----> made\n",
      "8058 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKf5sxFo6FPY"
   },
   "source": [
    "evenxyz에서 even을 독립적으로 분리하고 xyz는 훈련 데이터에서 하나의 단어로서 등장한 적이 없으므로 각각 전부 분리됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddhuLCuR6FPY"
   },
   "source": [
    "---\n",
    "### **2. 네이버 영화 리뷰 토큰화하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Se3bDGsAn8eu"
   },
   "source": [
    "네이버 영화 리뷰에 대해서도 위에서 IMDB 영화 리뷰에 대해서 수행한 동일한 작업을 진행해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea1Q3VuCaZTJ",
    "outputId": "382dc32d-16e6-4c8e-ef8a-bae3a18ec442"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings_test.txt', <http.client.HTTPMessage at 0x1f16e3e8520>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhmGUSI56FPZ"
   },
   "source": [
    "다운로드한 데이터를 데이터프레임에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PcCduq1rbUso"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Mc6zn1tVbVt_",
    "outputId": "6990869e-2dd6-42cd-8b2b-64e8191b7376"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>6222902</td>\n",
       "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>8549745</td>\n",
       "      <td>평점이 너무 낮아서...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>9311800</td>\n",
       "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>2376369</td>\n",
       "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>9619869</td>\n",
       "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
       "...          ...                                                ...    ...\n",
       "149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n",
       "149996   8549745                                      평점이 너무 낮아서...      1\n",
       "149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
       "149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
       "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
       "\n",
       "[150000 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "pvOXgs_N6FPa",
    "outputId": "3203f5d2-fc1a-49f1-fa8d-3221ef0e9488"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9274899</td>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6723715</td>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>4608761</td>\n",
       "      <td>오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 육쾌함</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>5308387</td>\n",
       "      <td>의지 박약들이나 하는거다 탈영은 일단 주인공 김대희 닮았고 이등병 찐따 OOOO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>9072549</td>\n",
       "      <td>그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>5802125</td>\n",
       "      <td>절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>6070594</td>\n",
       "      <td>마무리는 또 왜이래</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                           document  label\n",
       "0      6270596                                                굳 ㅋ      1\n",
       "1      9274899                               GDNTOPCLASSINTHECLUB      0\n",
       "2      8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
       "3      6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
       "4      6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0\n",
       "...        ...                                                ...    ...\n",
       "49995  4608761          오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 육쾌함      1\n",
       "49996  5308387       의지 박약들이나 하는거다 탈영은 일단 주인공 김대희 닮았고 이등병 찐따 OOOO      0\n",
       "49997  9072549                 그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다      0\n",
       "49998  5802125     절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네      0\n",
       "49999  6070594                                         마무리는 또 왜이래      0\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggCryHaz6FPa"
   },
   "source": [
    "이 데이터에는 Null 값이 존재하므로 이를 제거해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "fYN-HS5G6FPa",
    "outputId": "cdfad4b1-3da5-4b6d-91b0-086a32bc5a78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id     document  label\n",
       "False  False     False    149995\n",
       "       True      False         5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7i7tXANbn1J",
    "outputId": "b04309a6-7a7b-4695-d585-deaded5871ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PVOo0v5EbqIY",
    "outputId": "18f56556-10c0-4bbe-8c80-4db0ba6f01ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          0\n",
      "document    5\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BoywPnTbsNx",
    "outputId": "a376a7e3-0226-43ef-ca59-57bd92ebf782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGqFxim-6FPb"
   },
   "source": [
    "tfds.features.text.SubwordTextEncoder.build_from_corpus의 인자로 네이버 영화 리뷰 데이터를 넣어서, 서브워드들로 이루어진 단어 집합(Vocabulary)를 생성하고, 각 서브워드에 고유한 정수를 부여합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIdLXwqkcvAh"
   },
   "source": [
    "The vocabulary is \"trained\" on a corpus and all wordpieces are stored in a vocabulary file. To generate a vocabulary from a corpus, use tfds.features.text.SubwordTextEncoder.build_from_corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "t_mcMEyFbYsn"
   },
   "outputs": [],
   "source": [
    "### \n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    train_data['document'], target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "sm4wauqW6FPc"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_datasets.core.features' has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:2\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_datasets.core.features' has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### \n",
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    train_data['document'], target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nVMk3IA6FPc"
   },
   "source": [
    "토큰화 된 100개의 서브워드들을 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HoRiLWumhdHA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['. ', '..', '영화', '이_', '...', '의_', '는_', '도_', '다', ', ', '을_', '고_', '은_', '가_', '에_', '.. ', '한_', '너무_', '정말_', '를_', '고', '게_', '영화_', '지', '... ', '진짜_', '이', '다_', '요', '만_', '? ', '과_', '나', '가', '서_', '지_', '로_', '으로_', '아', '어', '....', '음', '한', '수_', '와_', '도', '네', '그냥_', '나_', '더_', '왜_', '이런_', '면_', '기', '하고_', '보고_', '하는_', '서', '좀_', '리', '자', '스', '안', '! ', '에서_', '영화를_', '미', 'ㅋㅋ', '네요', '시', '주', '라', '는', '오', '없는_', '에', '해', '사', '!!', '영화는_', '마', '잘_', '수', '영화가_', '만', '본_', '로', '그_', '지만_', '대', '은', '비', '의', '일', '개', '있는_', '없다', '함', '구', '하']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.subwords[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daUUyNgW6FPc"
   },
   "source": [
    "encode()를 통해 임의로 선택한 20번 인덱스의 샘플을 출력해보고, 정수 인코딩을 수행한 결과와 비교해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "IXdEoOHx3U3r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나름 심오한 뜻도 있는 듯. 그냥 학생이 선생과 놀아나는 영화는 절대 아님\n"
     ]
    }
   ],
   "source": [
    "print(train_data['document'][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "R7E8qlK2bdqX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sample question: [669, 4700, 17, 1749, 8, 96, 131, 1, 48, 2239, 4, 7466, 32, 1274, 2655, 7, 80, 749, 1254]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sample question: {}'.format(tokenizer.encode(train_data['document'][20])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOuiC66W6FPd"
   },
   "source": [
    "21번 인덱스 샘플에 대해서 정수 인코딩 결과를 확인하고, 이를 다시 역으로 디코딩해보겠습니다. 디코딩 할 때는 인코딩할 때 encode()를 사용한 것과 유사하게 decode()를 통해서 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "oV8H8R9Xb8Eu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 [570, 892, 36, 584, 159, 7091, 201]\n",
      "기존 문장: 보면서 웃지 않는 건 불가능하다\n"
     ]
    }
   ],
   "source": [
    "sample_string = train_data['document'][21]\n",
    "\n",
    "# encode\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# encoding한 문장을 다시 decode\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "U-dBPUHWderE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570 ----> 보면서 \n",
      "892 ----> 웃\n",
      "36 ----> 지 \n",
      "584 ----> 않는 \n",
      "159 ----> 건 \n",
      "7091 ----> 불가능\n",
      "201 ----> 하다\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "v_0X1RqrifJX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 [4, 23, 1364, 2157, 8235, 8128, 8130, 8235, 8147, 8169, 8235, 8147, 8169, 393]\n",
      "기존 문장: 이 영화 굉장히 재밌다 킄핫핫ㅎ\n"
     ]
    }
   ],
   "source": [
    "sample_string = '이 영화 굉장히 재밌다 킄핫핫ㅎ'\n",
    "\n",
    "# encode\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# encoding한 문장을 다시 decode\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ZwJDOhEOimwo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ----> 이 \n",
      "23 ----> 영화 \n",
      "1364 ----> 굉장히 \n",
      "2157 ----> 재밌다 \n",
      "8235 ----> �\n",
      "8128 ----> �\n",
      "8130 ----> �\n",
      "8235 ----> �\n",
      "8147 ----> �\n",
      "8169 ----> �\n",
      "8235 ----> �\n",
      "8147 ----> �\n",
      "8169 ----> �\n",
      "393 ----> ㅎ\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "17VccvjyL-iz2cWPYCE5IpeS5y0lmkS_i",
     "timestamp": 1674498833218
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
