{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8_43unDo-sb"
   },
   "source": [
    "이 자료는 위키독스 딥 러닝을 이용한 자연어 처리 입문의 허깅페이스 토크나이저 학습 자료입니다.  \n",
    "\n",
    "링크 : https://wikidocs.net/99893"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-bMTBE-SLI"
   },
   "source": [
    "## **13-04 허깅페이스 토크나이저(Huggingface Tokenizer)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nyr4Ma10-SLI"
   },
   "source": [
    "자연어 처리 스타트업 허깅페이스가 개발한 패키지 tokenizers는 자주 등장하는 서브워드들을 하나의 토큰으로 취급하는 다양한 서브워드 토크나이저를 제공합니다. 이번 실습에서는 이 중에서 WordPiece Tokenizer를 실습해보겠습니다. 실습을 위해 우선 tokenizers를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5276,
     "status": "ok",
     "timestamp": 1674551866745,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "OuyGviaGpdTC",
    "outputId": "05312e57-3f75-4975-83db-ca626b823531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\jeong\\anaconda3\\envs\\cb_tf25_py38\\lib\\site-packages (0.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\jeong\\anaconda3\\envs\\cb_tf25_py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\jeong\\anaconda3\\envs\\cb_tf25_py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\jeong\\anaconda3\\envs\\cb_tf25_py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\jeong\\anaconda3\\envs\\cb_tf25_py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\jeong\\anaconda3\\envs\\cb_tf25_py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\jeong\\anaconda3\\envs\\cb_tf25_py38\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM5ahbJ5pHYp"
   },
   "source": [
    "### **1. BERT의 워드피스 토크나이저(BertWordPieceTokenizer)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BECvDrrh-SLL"
   },
   "source": [
    "구글이 공개한 딥 러닝 모델 BERT에는 WordPiece Tokenizer가 사용되었습니다. 허깅페이스는 해당 토크나이저를 직접 구현하여 tokenizers라는 패키지를 통해 버트워드피스토크나이저(BertWordPieceTokenizer)를 제공합니다.\n",
    "\n",
    "여기서는 네이버 영화 리뷰 데이터를 해당 토크나이저에 학습시키고, 이로부터 서브워드의 단어 집합(Vocabulary)을 얻습니다. 그리고 임의의 문장에 대해서 학습된 토크나이저를 사용하여 토큰화를 진행합니다. 우선 네이버 영화 리뷰 데이터를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 457,
     "status": "ok",
     "timestamp": 1674551870714,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "I6y5vrE1qyH-",
    "outputId": "9b5d6eda-58c4-4b40-cf3e-9ca6003c05b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "tokenizers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 974,
     "status": "ok",
     "timestamp": 1674551873253,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "PjIo8gG7pir-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 718,
     "status": "ok",
     "timestamp": 1674551876280,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "yy6bJkk-qEqG",
    "outputId": "3cf0e0af-8ec2-46fd-e194-db9ec08c07bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings.txt', <http.client.HTTPMessage at 0x272bfad6070>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "executionInfo": {
     "elapsed": 1225,
     "status": "ok",
     "timestamp": 1674551888752,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "eDRGo02FqEz9",
    "outputId": "726f0928-d4a8-4498-b1ef-cb3b53714be9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>8963373</td>\n",
       "      <td>포켓 몬스터 짜가 ㅡㅡ;;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>3302770</td>\n",
       "      <td>쓰.레.기</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>5458175</td>\n",
       "      <td>완전 사이코영화. 마지막은 더욱더 이 영화의질을 떨어트린다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>6908648</td>\n",
       "      <td>왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>8548411</td>\n",
       "      <td>포풍저그가나가신다영차영차영차</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1        8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2        4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3        9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4       10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
       "...          ...                                                ...    ...\n",
       "199995   8963373                                     포켓 몬스터 짜가 ㅡㅡ;;      0\n",
       "199996   3302770                                              쓰.레.기      0\n",
       "199997   5458175                  완전 사이코영화. 마지막은 더욱더 이 영화의질을 떨어트린다.      0\n",
       "199998   6908648                왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ      0\n",
       "199999   8548411                                    포풍저그가나가신다영차영차영차      0\n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naver_df = pd.read_table('ratings.txt')\n",
    "# naver_df = pd.read_csv('ratings.txt', delimiter='\\t')\n",
    "naver_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7hs42-Y-SLM"
   },
   "source": [
    "센텐스피스(SentencePiece) 실습에서 진행했던 전처리와 동일한 과정을 진행합니다. ratings.txt라는 파일을 데이터프레임으로 로드한 후, 결측값을 제거하고, 실질적인 리뷰 데이터인 document열에 대해서 naver_review.txt라는 파일로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1674551904253,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "e_uODnAr-SLM",
    "outputId": "118f2a0b-6fa5-40c9-dc46-926a7743967e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id     document  label\n",
       "False  False     False    199992\n",
       "       True      False         8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naver_df.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1674551914450,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "PuNq2qiHqWK7",
    "outputId": "83a8d089-60c0-4ee3-c502-1fff5a4f7929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "naver_df = naver_df.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(naver_df.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1674551922410,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "R6oyRgzuqG0O"
   },
   "outputs": [],
   "source": [
    "with open('naver_review.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(naver_df['document']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmG3ouFI-SLN"
   },
   "source": [
    "버트워드피스토크나이저를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1674551926470,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "UUxat59Kr_HN"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGak2nRzz1VF"
   },
   "source": [
    "각 인자가 의미하는 바는 다음과 같습니다.\n",
    "\n",
    "* __lowercase__ : True일 경우 토크나이저는 영어의 대문자와 소문자를 동일한 문자 취급.  \n",
    "* __strip_accents__ : True일 경우 악센트 제거.  \n",
    "     ex) é → e, ô → o  \n",
    "* __wordpieces_prefix__ : 서브워드로 쪼개졌을 경우 뒤의 서브워드에는 ##를 부착하여 원래 단어에서 분리된 것임을 표시.  \n",
    "     ex) 안녕하세요 -> [안녕, ##하세요]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8pLOQra-SLO"
   },
   "source": [
    "네이버 영화 리뷰 데이터를 학습하여 단어 집합을 얻어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 10588,
     "status": "ok",
     "timestamp": 1674551974779,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "RqfckZaKsB52"
   },
   "outputs": [],
   "source": [
    "data_file = 'naver_review.txt'\n",
    "vocab_size = 30000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "tokenizer.train(files=data_file,\n",
    "                vocab_size=vocab_size,\n",
    "                limit_alphabet=limit_alphabet,\n",
    "                min_frequency=min_frequency,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jub515LugVN"
   },
   "source": [
    "각 인자가 의미하는 바는 다음과 같습니다.\n",
    "\n",
    "* files : 단어 집합을 얻기 위해 학습할 데이터\n",
    "* vocab_size : 단어 집합의 크기\n",
    "* limit_alphabet : 병합 전의 초기 토큰의 허용 개수.  \n",
    "* min_frequency : 최소 해당 횟수만큼 등장한 쌍(pair)의 경우에만 병합 대상이 된다.  \n",
    "  \n",
    "학습이 다 되었다면 vocab을 저장합니다. 경로를 지정해주어야 하는데 여기서는 현재 경로에 저장하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1674551982821,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "roBgrAOpvvR0",
    "outputId": "9740c714-7442-46f7-b5e5-837642d28fa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vocab.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab 저장\n",
    "tokenizer.save_model('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqHK31t6-SLP"
   },
   "source": [
    "vocab을 데이터프레임으로 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1674552338891,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "xZKTKl_SF1Zu",
    "outputId": "1e0c530a-8e34-4288-b02e-267ae2cc7b37"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UNK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MASK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>말안</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>말들이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>말라는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>말밖에는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>맘을</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0       [PAD]\n",
       "1       [UNK]\n",
       "2       [CLS]\n",
       "3       [SEP]\n",
       "4      [MASK]\n",
       "...       ...\n",
       "29995      말안\n",
       "29996     말들이\n",
       "29997     말라는\n",
       "29998    말밖에는\n",
       "29999      맘을\n",
       "\n",
       "[30000 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab 로드  (fwf : fixed width formatted lines)\n",
    "df = pd.read_fwf('vocab.txt', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adj_Cqzp-SLQ"
   },
   "source": [
    "총 30,000개의 단어가 존재합니다. 이는 단어 집합의 크기를 30,000으로 지정하였기 때문입니다. 실제 토큰화를 수행해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1674552190204,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "RtXbcZHasGxO",
    "outputId": "3208cc80-529c-4d5c-f41b-48e5a674a19f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 : ['아', '배고', '##픈', '##데', '짜장면', '##먹고', '##싶다']\n",
      "정수 인코딩 : [2111, 20632, 3988, 3279, 24682, 7871, 7379]\n",
      "디코딩 : 아 배고픈데 짜장면먹고싶다\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('아 배고픈데 짜장면먹고싶다')\n",
    "print('토큰화 결과 :',encoded.tokens)\n",
    "print('정수 인코딩 :',encoded.ids)\n",
    "print('디코딩 :',tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYOlq93B-SLR"
   },
   "source": [
    ".ids는 실질적인 딥 러닝 모델의 입력으로 사용되는 정수 인코딩 결과를 출력합니다. tokens는 해당 토크나이저가 어떻게 토큰화를 진행했는지를 보여줍니다. decode()는 정수 시퀀스를 문자열로 복원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1674552194907,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "FYQwqZajsqKs",
    "outputId": "c6683cd9-379d-4f56-da94-ce99d995f37c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 : ['커피', '한잔', '##의', '여유', '##를', '즐기', '##다']\n",
      "정수 인코딩 : [12825, 25648, 3314, 12696, 3323, 10784, 3346]\n",
      "디코딩 : 커피 한잔의 여유를 즐기다\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('커피 한잔의 여유를 즐기다')\n",
    "print('토큰화 결과 :',encoded.tokens)\n",
    "print('정수 인코딩 :',encoded.ids)\n",
    "print('디코딩 :',tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gA4GhI1apI5r"
   },
   "source": [
    "### **2. 기타 토크나이저**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzG8u0E1-SLR"
   },
   "source": [
    "이 외 ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer 등이 존재하며 선택에 따라서 사용할 수 있습니다.\n",
    "\n",
    "* __BertWordPieceTokenizer__ : BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer)\n",
    "* __CharBPETokenizer__ : 오리지널 BPE\n",
    "* __ByteLevelBPETokenizer__ : BPE의 바이트 레벨 버전\n",
    "* __SentencePieceBPETokenizer__ : 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1674552200494,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "mkgJ-USR4cg9"
   },
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16754,
     "status": "ok",
     "timestamp": 1674552218688,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "0vY2KY3q-SLR",
    "outputId": "61fe2745-50ee-4c36-83f8-7f356a44c4e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁이', '▁영화는', '▁정말', '▁재미있', '습니다.']\n"
     ]
    }
   ],
   "source": [
    "## SentencePieceBPETokenizer\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train('naver_review.txt', vocab_size=10000, min_frequency=5)\n",
    "\n",
    "encoded = tokenizer.encode(\"이 영화는 정말 재미있습니다.\")\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144874,
     "status": "ok",
     "timestamp": 1674552869490,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "sMeEOl2g-SLS",
    "outputId": "7474593a-2669-4139-c81c-05f64c3159d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ìĿ´', 'ĠìĺģíĻĶëĬĶ', 'Ġìłķë§Ĳ', 'Ġìŀ¬ë¯¸ìŀĪìĬµëĭĪëĭ¤', '.']\n"
     ]
    }
   ],
   "source": [
    "# ByteLevelBPETokenizer\n",
    "bl_tokenizer = ByteLevelBPETokenizer()\n",
    "# bl_tokenizer = ByteLevelBPETokenizer(unicode_normalizer=\"nfkc\", trim_offsets=True)\n",
    "bl_tokenizer.train('naver_review.txt', vocab_size=10000, min_frequency=5)\n",
    "\n",
    "encoded = bl_tokenizer.encode(\"이 영화는 정말 재미있습니다.\")\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7996,
     "status": "ok",
     "timestamp": 1674552877480,
     "user": {
      "displayName": "Jeong-Ho JEON (전정호)",
      "userId": "09770030589626015137"
     },
     "user_tz": -540
    },
    "id": "-XfWSEjo-SLS",
    "outputId": "127dac7c-38a6-449f-f7fd-f8bd310b3cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이</w>', '영화는</w>', '정말</w>', '재미있습니다</w>', '.</w>']\n"
     ]
    }
   ],
   "source": [
    "# CharBPETokenizer\n",
    "cb_tokenizer = CharBPETokenizer()\n",
    "cb_tokenizer.train('naver_review.txt', vocab_size=10000, min_frequency=5)\n",
    "\n",
    "encoded = cb_tokenizer.encode(\"이 영화는 정말 재미있습니다.\")\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iysLpGBm-SLS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaNtisToIvWO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXn8AsC4I1Fq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "17UvrTfGYxgW4MCS4DHuECWhcF1XsA3aC",
     "timestamp": 1674499998868
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
